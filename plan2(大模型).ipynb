{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b087554b-34da-4667-810f-2e710161db06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 库与数据探索and 观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7396b777-2d59-477c-97c9-29db3ea52775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436694c2-e23f-45da-868a-0fa0f6d35e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 当前设备： NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1. 设置数据路径与设备\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_multitask_dataset.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes.json\"\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(\"✅ 当前设备：\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf099c9-1f50-4a60-b184-939d1d63681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 2. 读取数据\n",
    "df_all = pd.read_csv(csv_path)\n",
    "#df_sample = df_all.sample(n=20000, random_state=42)  # 小样本验证\n",
    "# 保存这部分用于建模的数据\n",
    "#df_sample.to_csv(\"plant_sample_20k.csv\", index=False)\n",
    "\n",
    "# 2. 从抽样数据中生成 organ 和 species 编码映射\n",
    "#organ_classes = {name: idx for idx, name in enumerate(sorted(df_sample['organ'].unique()))}\n",
    "#species_classes = {name: idx for idx, name in enumerate(sorted(df_sample['label'].unique()))}\n",
    "\n",
    "# 3. 添加编码列\n",
    "#df_sample['organ_id'] = df_sample['organ'].map(organ_classes)\n",
    "#df_sample['species_id'] = df_sample['label'].map(species_classes)\n",
    "\n",
    "# 4. 保存数据与映射\n",
    "'''df_sample.to_csv(\"plant_subset_20k.csv\", index=False)\n",
    "with open(\"organ_classes_20k.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(organ_classes, f, ensure_ascii=False, indent=2)\n",
    "with open(\"species_classes_20k.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(species_classes, f, ensure_ascii=False, indent=2)'''\n",
    "\n",
    "# 必须重新编码标签\n",
    "species_classes = {name: idx for idx, name in enumerate(sorted(df_all['label'].unique()))}\n",
    "df_all['species_id'] = df_all['label'].map(species_classes)\n",
    "df_all = df_all[df_all['species_id'].notnull()].copy()\n",
    "df_all['species_id'] = df_all['species_id'].astype(int)\n",
    "\n",
    "# 检查标签值范围\n",
    "num_classes = len(species_classes)\n",
    "assert df_all['species_id'].min() >= 0\n",
    "assert df_all['species_id'].max() < num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f47141-9d27-49de-9e49-f2ca9c8f8503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>species_id_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spathoglottis pubescens</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>3058</td>\n",
       "      <td>3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label   organ  \\\n",
       "0  Spathoglottis pubescens  flower   \n",
       "1        Phoenix loureiroi    leaf   \n",
       "2        Phoenix loureiroi    leaf   \n",
       "3        Phoenix loureiroi    leaf   \n",
       "4        Phoenix loureiroi    leaf   \n",
       "\n",
       "                                          image_path  organ_id  species_id  \\\n",
       "0  /mnt/zshare/plants/Plant_Data/top5000_china/Sp...         1        3058   \n",
       "1  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "2  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "3  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "4  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "\n",
       "   species_id_masked  \n",
       "0               3058  \n",
       "1               2428  \n",
       "2               2428  \n",
       "3               2428  \n",
       "4               2428  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739f5a68-a20b-493c-9ffe-e291f6855684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                   3467\n",
       "organ                      4\n",
       "image_path           1089470\n",
       "organ_id                   4\n",
       "species_id              3467\n",
       "species_id_masked       3444\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5237aa8d-c994-4a85-a89f-da1d11ec5fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "organ\n",
      "leaf      531127\n",
      "flower    417738\n",
      "fruit      74864\n",
      "bark       65741\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_all['organ_id'].nunique())\n",
    "print(df_all['organ'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01ee52-a174-47c1-b135-902945a4e5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7eee6dc-e2f0-40f2-9c44-d084ca1f7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "# 选择其中一个路径作为 font_path\n",
    "font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"  # Noto 中文\n",
    "\n",
    "\n",
    "# 注册字体\n",
    "my_font = font_manager.FontProperties(fname=font_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23b3add-bdd9-4dc4-9045-629aab5d486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Acer negundo               1000\n",
      "Caesalpinia pulcherrima     999\n",
      "Elaeagnus pungens           999\n",
      "Sorbus aucuparia            999\n",
      "Acer rubrum                 999\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_font' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(species_counts\u001b[38;5;241m.\u001b[39mvalues, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m图像数量\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontproperties\u001b[38;5;241m=\u001b[39m\u001b[43mmy_font\u001b[49m)\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m物种数量\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontproperties\u001b[38;5;241m=\u001b[39mmy_font)\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m每个植物物种的图像数量分布\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontproperties\u001b[38;5;241m=\u001b[39mmy_font)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_font' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#数据统计与可视化\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 统计每个植物物种的图像数量\n",
    "species_counts = df_all['label'].value_counts()\n",
    "\n",
    "# 显示前20个植物的图像数（可选）\n",
    "print(species_counts.head())\n",
    "\n",
    "# 可视化图像数量分布（直方图）\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(species_counts.values, bins=50)\n",
    "plt.xlabel(\"图像数量\", fontproperties=my_font)\n",
    "plt.ylabel(\"物种数量\", fontproperties=my_font)\n",
    "plt.title(\"每个植物物种的图像数量分布\", fontproperties=my_font)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dae25409-4618-4f2d-a978-996aae099f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Noto Sans CJK\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 统计各器官图像数量\n",
    "organ_counts = df_all['organ'].value_counts()\n",
    "\n",
    "# 条形图显示\n",
    "plt.figure(figsize=(8, 5))\n",
    "organ_counts.plot(kind='bar')\n",
    "plt.ylabel(\"图像数量\", fontproperties=my_font)\n",
    "plt.title(\"各器官图像分布\", fontproperties=my_font)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec2483-5ca8-4305-af7e-96a38d41349e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### resnet-50-----------------------------train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0288fc4a-d063-4f04-9d1b-a4707eb643c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PlantDataset object at 0x7cf870123dc0>\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #return image, row['organ_label'], row['species_id']\n",
    "        label = int(row['species_id'])\n",
    "        return image, label\n",
    "        \n",
    "dataset = PlantDataset(df_all, transform)\n",
    "#loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4405ca84-3783-4895-8ebd-fc18788aa31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3467.000000\n",
      "mean      314.239977\n",
      "std       349.867180\n",
      "min         1.000000\n",
      "25%        12.000000\n",
      "50%       115.000000\n",
      "75%       622.000000\n",
      "max      1000.000000\n",
      "Name: count, dtype: float64\n",
      "3467\n"
     ]
    }
   ],
   "source": [
    "print(df_all['species_id'].value_counts().describe())\n",
    "print(df_all['species_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1ff8854-7306-4623-80c2-2a85c5605655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>species_id_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spathoglottis pubescens</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>3058</td>\n",
       "      <td>3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label   organ  \\\n",
       "0  Spathoglottis pubescens  flower   \n",
       "1        Phoenix loureiroi    leaf   \n",
       "2        Phoenix loureiroi    leaf   \n",
       "3        Phoenix loureiroi    leaf   \n",
       "4        Phoenix loureiroi    leaf   \n",
       "\n",
       "                                          image_path  organ_id  species_id  \\\n",
       "0  /mnt/zshare/plants/Plant_Data/top5000_china/Sp...         1        3058   \n",
       "1  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "2  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "3  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "4  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "\n",
       "   species_id_masked  \n",
       "0               3058  \n",
       "1               2428  \n",
       "2               2428  \n",
       "3               2428  \n",
       "4               2428  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da6b279-a8a8-492a-8d62-b9c4012f5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89347f8-e618-49c8-8162-73adc1c174f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "\n",
    "#num_classes = len(species_classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "#model.fc = nn.Linear(model.fc.in_features, len(species_classes))  # 修改输出层\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)  # ✅ 将模型放到 CUDA 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cc9b87-49f0-44cf-bd96-ec831c542027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 标签最小值： 0\n",
      "✅ 标签最大值： 3466\n",
      "✅ 类别总数（模型输出）： 3467\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ 标签最小值：\", df_all['species_id'].min())\n",
    "print(\"✅ 标签最大值：\", df_all['species_id'].max())\n",
    "print(\"✅ 类别总数（模型输出）：\", len(species_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366586a6-a5cb-4fe6-a028-a86f7057ce36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1/2:   0%|                     | 91/27237 [03:15<16:12:46,  2.15s/it, acc=0.000687, loss=8.06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ 1. 初始化\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ✅ 2. 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, labels in train_pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_pbar.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "    avg_train_loss = train_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # ✅ 3. 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_pbar = tqdm(val_loader, desc=f\"[Val]   Epoch {epoch+1}/{num_epochs}\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_pbar.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "    avg_val_loss = val_loss / total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36468ab-3591-44d7-8c46-12d9caf0fdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78ba74ef-780b-4c0f-a45a-9f8bacfd70d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 大模型尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42c96d-7362-48c7-9801-26efc0db6cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting open_clip_torch\n",
      "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from open_clip_torch) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from open_clip_torch) (0.20.1)\n",
      "Collecting regex (from open_clip_torch)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting ftfy (from open_clip_torch)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tqdm in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from open_clip_torch) (4.67.1)\n",
      "Collecting huggingface-hub (from open_clip_torch)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors (from open_clip_torch)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting timm (from open_clip_torch)\n",
      "  Downloading timm-1.0.17-py3-none-any.whl.metadata (59 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from faiss-cpu) (2.0.1)\n",
      "Requirement already satisfied: packaging in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
      "Collecting fsspec (from torch>=1.9.0->open_clip_torch)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.9.0->open_clip_torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub->open_clip_torch)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.7.14)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages (from torchvision->open_clip_torch) (11.3.0)\n",
      "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m737.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading timm-1.0.17-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, safetensors, regex, hf-xet, ftfy, fsspec, faiss-cpu, huggingface-hub, timm, open_clip_torch\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.13.3\n",
      "\u001b[2K    Uninstalling sympy-1.13.3:\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.3━━━━━\u001b[0m \u001b[32m 0/10\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [open_clip_torch] [open_clip_torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed faiss-cpu-1.11.0.post1 fsspec-2025.7.0 ftfy-6.3.1 hf-xet-1.1.5 huggingface-hub-0.33.4 open_clip_torch-2.32.0 regex-2024.11.6 safetensors-0.5.3 sympy-1.13.1 timm-1.0.17\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install open_clip_torch faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3415b23a-08e6-4b28-98fc-6cf9dc52d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_multitask_dataset.csv\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes.json\"\n",
    "\n",
    "# 加载数据与类别映射\n",
    "df_all = pd.read_csv(csv_path)\n",
    "with open(species_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "# 反向映射：id -> name\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff980d3-037b-404a-80ce-473a0779b65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>species_id_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spathoglottis pubescens</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>3058</td>\n",
       "      <td>3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label   organ  \\\n",
       "0  Spathoglottis pubescens  flower   \n",
       "1        Phoenix loureiroi    leaf   \n",
       "2        Phoenix loureiroi    leaf   \n",
       "3        Phoenix loureiroi    leaf   \n",
       "4        Phoenix loureiroi    leaf   \n",
       "\n",
       "                                          image_path  organ_id  species_id  \\\n",
       "0  /mnt/zshare/plants/Plant_Data/top5000_china/Sp...         1        3058   \n",
       "1  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "2  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "3  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "4  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "\n",
       "   species_id_masked  \n",
       "0               3058  \n",
       "1               2428  \n",
       "2               2428  \n",
       "3               2428  \n",
       "4               2428  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35b23ef-b67d-4293-a5c0-f04096e01641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848af1ff81614de8b2220d556918289a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion2b_s34b_b79k'\n",
    ")\n",
    "model = model.to(device).eval()\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542fdbb4-899e-4e7b-be43-5968985e0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1089470/1089470 [4:41:25<00:00, 64.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "for idx, row in tqdm(df_all.iterrows(), total=len(df_all)):\n",
    "    try:\n",
    "        image = preprocess(Image.open(row[\"image_path\"]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        all_embeddings.append(image_features.cpu().numpy())\n",
    "        all_labels.append(row[\"species_id\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 错误跳过 {row['image_path']}：{e}\")\n",
    "        continue\n",
    "\n",
    "emb_array = np.vstack(all_embeddings).astype(\"float32\")\n",
    "label_array = np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8b3b79-462b-41ff-8b03-7fda369e4d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ faiss 索引建立完成，向量数量： 1089470\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(emb_array.shape[1])  # 使用内积衡量相似度（向量已归一化）\n",
    "index.add(emb_array)\n",
    "print(\"✅ faiss 索引建立完成，向量数量：\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72890b5f-62c1-4458-a6a0-58b2145017f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_species(image_path, topk=5):\n",
    "    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    image_features = image_features.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    D, I = index.search(image_features, topk)\n",
    "    predictions = [(id_to_species[label_array[i]], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "325755c8-a0c0-46e4-9463-dedd19fec6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 推理结果 Top-3：\n",
      "Viburnum opulus  相似度：0.8671\n",
      "Liriodendron chinense  相似度：0.8638\n",
      "Lonicera tatarica  相似度：0.8629\n"
     ]
    }
   ],
   "source": [
    "test_img = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "results = predict_species(test_img, topk=3)\n",
    "\n",
    "print(\"🌿 推理结果 Top-3：\")\n",
    "for name, score in results:\n",
    "    print(f\"{name}  相似度：{score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98ebb81-17c0-4232-a04a-5fdc5bc722dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>species_id_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spathoglottis pubescens</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>3058</td>\n",
       "      <td>3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phoenix loureiroi</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ph...</td>\n",
       "      <td>3</td>\n",
       "      <td>2428</td>\n",
       "      <td>2428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label   organ  \\\n",
       "0  Spathoglottis pubescens  flower   \n",
       "1        Phoenix loureiroi    leaf   \n",
       "2        Phoenix loureiroi    leaf   \n",
       "3        Phoenix loureiroi    leaf   \n",
       "4        Phoenix loureiroi    leaf   \n",
       "\n",
       "                                          image_path  organ_id  species_id  \\\n",
       "0  /mnt/zshare/plants/Plant_Data/top5000_china/Sp...         1        3058   \n",
       "1  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "2  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "3  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "4  /mnt/zshare/plants/Plant_Data/top5000_china/Ph...         3        2428   \n",
       "\n",
       "   species_id_masked  \n",
       "0               3058  \n",
       "1               2428  \n",
       "2               2428  \n",
       "3               2428  \n",
       "4               2428  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52b889-c39b-4ec1-8a36-b09534918925",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 二阶段模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "667c02d0-bc98-4e67-ba9a-b056b3714cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本器官示例: ['flower' 'leaf' 'bark' 'fruit']\n",
      "organ_classes 键: ['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "print(\"样本器官示例:\", df['organ'].unique())\n",
    "print(\"organ_classes 键:\", list(organ_classes.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466390e-69d3-434f-9005-dee1f91fd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1: 100%|███████████████████████████████| 17023/17023 [2:39:37<00:00,  1.78it/s, acc=0.814, loss=0.511]\n",
      "Epoch 2: 100%|███████████████████████████████| 17023/17023 [2:38:27<00:00,  1.79it/s, acc=0.836, loss=0.453]\n",
      "Epoch 3: 100%|███████████████████████████████| 17023/17023 [2:39:44<00:00,  1.78it/s, acc=0.852, loss=0.411]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ecb2cea46c4a40aa4c4cc311db92eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "Embedding bark:   3%|█▍                                                | 1955/65741 [00:49<27:46, 38.27it/s]"
     ]
    }
   ],
   "source": [
    "# ✅ 两阶段植物识别系统：OrganClassifier + CLIP Species Retrieval\n",
    "# 适用于 plant_multitask_dataset.csv 格式数据，字段包括 path, organ, label\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== Step 1: 初始化设备与数据 ======\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_multitask_dataset.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes_20k.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes_full.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "organ_classes = {str(k).strip().lower(): v for k, v in organ_classes.items()}\n",
    "    \n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "\n",
    "# ====== Step 2: 构建 OrganClassifier 数据集与模型 ======\n",
    "class OrganDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        #label_name = str(row['organ'])\n",
    "        label_name = str(row['organ']).strip().lower()\n",
    "\n",
    "        if label_name not in organ_classes:\n",
    "            raise ValueError(f\"未知的器官标签: {label_name}\")\n",
    "        label = organ_classes[label_name]\n",
    "        return image, label\n",
    "\n",
    "# 数据增强\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "organ_dataset = OrganDataset(df, organ_transform)\n",
    "organ_loader = DataLoader(organ_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 构建模型\n",
    "organ_model = models.resnet18(pretrained=True)\n",
    "organ_model.fc = nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model = organ_model.to(device)\n",
    "\n",
    "# 训练器官分类器（可加早停）\n",
    "optimizer = torch.optim.Adam(organ_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "acc_list, loss_list = [], []\n",
    "for epoch in range(3):  # 可调轮数\n",
    "    organ_model.train()\n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0\n",
    "    loop = tqdm(organ_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = organ_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=running_loss/total)\n",
    "    acc_list.append(correct / total)\n",
    "    loss_list.append(running_loss / total)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc_list, label='Accuracy')\n",
    "plt.title('OrganClassifier Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_list, label='Loss', color='orange')\n",
    "plt.title('OrganClassifier Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== Step 3: 构建每器官的 CLIP 向量库 ======\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "organ_index = {}\n",
    "organ_label_array = {}\n",
    "\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    organ_df = df[df['organ'] == organ_name].reset_index(drop=True)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(organ_df.iterrows(), total=len(organ_df), desc=f\"Embedding {organ_name}\"):\n",
    "        try:\n",
    "            image = preprocess(Image.open(row['image_path']).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = clip_model.encode_image(image)\n",
    "                emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(species_classes[row['label']])\n",
    "        except:\n",
    "            continue\n",
    "    if embeddings:\n",
    "        emb_arr = np.vstack(embeddings).astype(\"float32\")\n",
    "        index = faiss.IndexFlatIP(emb_arr.shape[1])\n",
    "        index.add(emb_arr)\n",
    "        organ_index[organ_id] = index\n",
    "        organ_label_array[organ_id] = np.array(labels)\n",
    "\n",
    "# ====== Step 4: 两阶段推理函数 ======\n",
    "def predict_species(image_path, topk=5):\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 向量检索\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "    result = [(id_to_species[labels[i]], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "    return id_to_organ[pred_organ_id], result\n",
    "\n",
    "# ====== Step 5: 测试 ======\n",
    "img_path = \"/mnt/e/code/plants-classification-conda/images/test_leaf.jpg\"\n",
    "organ_pred, top_preds = predict_species(img_path)\n",
    "print(\"Predicted organ:\", organ_pred)\n",
    "for name, score in top_preds:\n",
    "   print(f\"{name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f715c70-068b-4882-b9d2-a0b8455ea8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a458a-45a0-4df0-95cb-7cdf87b5c3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a781986-d51e-49b7-9f58-f6e77a660182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00d1b5c6-d6e5-462f-953b-e26076ce69df",
   "metadata": {},
   "source": [
    "# 完全可用全流程模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fbbc9-9b53-4c8d-ae5c-72d14d8a48aa",
   "metadata": {},
   "source": [
    "## 植物种类分类模型的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba97c93-0cca-44e4-8f54-153e13cb0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9fddb88-998b-43fe-996f-3fe8c49a236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1:   7%|▋         | 27/390 [00:15<03:32,  1.71it/s, acc=0.649, loss=0.885]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     82\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(organ_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[1;32m     84\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     85\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m organ_model(images)\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[29], line 50\u001b[0m, in \u001b[0;36mOrganDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     49\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m---> 50\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#label_name = str(row['organ'])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/PIL/Image.py:3513\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m   3512\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m-> 3513\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3514\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 测试数据\n",
    "\n",
    "# ✅ 两阶段植物识别系统：OrganClassifier + CLIP Species Retrieval\n",
    "# 适用于 plant_multitask_dataset.csv 格式数据，字段包括 path, organ, label\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== Step 1: 初始化设备与数据 ======\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes_top997.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes_top997.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "organ_classes = {str(k).strip().lower(): v for k, v in organ_classes.items()}\n",
    "    \n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "\n",
    "# ====== Step 2: 构建 OrganClassifier 数据集与模型 ======\n",
    "class OrganDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        #label_name = str(row['organ'])\n",
    "        label_name = str(row['organ']).strip().lower()\n",
    "\n",
    "        if label_name not in organ_classes:\n",
    "            raise ValueError(f\"未知的器官标签: {label_name}\")\n",
    "        label = organ_classes[label_name]\n",
    "        return image, label\n",
    "\n",
    "# 数据增强\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "organ_dataset = OrganDataset(df, organ_transform)\n",
    "organ_loader = DataLoader(organ_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 构建模型\n",
    "organ_model = models.resnet18(pretrained=True)\n",
    "organ_model.fc = nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model = organ_model.to(device)\n",
    "\n",
    "# 训练器官分类器（可加早停）\n",
    "optimizer = torch.optim.Adam(organ_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "acc_list, loss_list = [], []\n",
    "for epoch in range(3):  # 可调轮数\n",
    "    organ_model.train()\n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0\n",
    "    loop = tqdm(organ_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = organ_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=running_loss/total)\n",
    "    acc_list.append(correct / total)\n",
    "    loss_list.append(running_loss / total)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc_list, label='Accuracy')\n",
    "plt.title('OrganClassifier Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_list, label='Loss', color='orange')\n",
    "plt.title('OrganClassifier Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== Step 3: 构建每器官的 CLIP 向量库 ======\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "organ_index = {}\n",
    "organ_label_array = {}\n",
    "\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    organ_df = df[df['organ'] == organ_name].reset_index(drop=True)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(organ_df.iterrows(), total=len(organ_df), desc=f\"Embedding {organ_name}\"):\n",
    "        try:\n",
    "            image = preprocess(Image.open(row['image_path']).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = clip_model.encode_image(image)\n",
    "                emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(species_classes[row['label']])\n",
    "        except:\n",
    "            continue\n",
    "    if embeddings:\n",
    "        emb_arr = np.vstack(embeddings).astype(\"float32\")\n",
    "        index = faiss.IndexFlatIP(emb_arr.shape[1])\n",
    "        index.add(emb_arr)\n",
    "        organ_index[organ_id] = index\n",
    "        organ_label_array[organ_id] = np.array(labels)\n",
    "\n",
    "# ====== Step 4: 两阶段推理函数 ======\n",
    "'''def predict_species(image_path, topk=5):\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 向量检索\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "    result = [(id_to_species[labels[i]], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "    return id_to_organ[pred_organ_id], result\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2a3af17c-19ae-4c00-b90a-e0d4a9dc3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted organ: leaf\n",
      "Berberis japonica: 0.7635\n",
      "Berberis japonica: 0.7502\n",
      "Acacia dealbata: 0.7442\n",
      "Sambucus racemosa: 0.7390\n",
      "Senecio vulgaris: 0.7363\n"
     ]
    }
   ],
   "source": [
    "# ====== Step 5: 测试 ======\n",
    "img_path = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "organ_pred, top_preds = predict_species(img_path)\n",
    "print(\"Predicted organ:\", organ_pred)\n",
    "for name, score in top_preds:\n",
    "   print(f\"{name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "29a546b5-16a0-452a-bb9c-a42580b33aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用top997数据训练植物分类模型\n",
    "#为了尽快测试功能1，2，3是否可行\n",
    "\n",
    "#这一段是存储植物分类模型\n",
    "torch.save(organ_model.state_dict(), \"organ_classifier_top997.pth\")\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.makedirs(\"clip_index_top997\", exist_ok=True)\n",
    "\n",
    "for organ_id, index in organ_index.items():\n",
    "    faiss.write_index(index, f\"clip_index_top997/index_{organ_id}.index\")\n",
    "    np.save(f\"clip_index_top997/labels_{organ_id}.npy\", organ_label_array[organ_id])\n",
    "\n",
    "\n",
    "with open(\"organ_classes_top997.json\", \"w\") as f:\n",
    "    json.dump(organ_classes, f)\n",
    "\n",
    "with open(\"species_classes_top997.json\", \"w\") as f:\n",
    "    json.dump(species_classes, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc027612-bec3-4ac5-9b88-f6ffdbf09e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31370/894423387.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  organ_model.load_state_dict(torch.load(\"organ_classifier_top997.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def predict_species(image_path, topk=5):\\n    # Step 1: 预测器官\\n    image = Image.open(image_path).convert(\"RGB\")\\n    img_tensor = organ_transform(image).unsqueeze(0).to(device)\\n    with torch.no_grad():\\n        pred_organ_id = organ_model(img_tensor).argmax(1).item()\\n\\n    # Step 2: CLIP 检索\\n    clip_img = preprocess(image).unsqueeze(0).to(device)\\n    with torch.no_grad():\\n        query_vec = clip_model.encode_image(clip_img)\\n        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\\n        query_np = query_vec.cpu().numpy().astype(\"float32\")\\n\\n    # Step 3: 相似度检索\\n    index = organ_index[pred_organ_id]\\n    labels = organ_label_array[pred_organ_id]\\n    D, I = index.search(query_np, topk)\\n\\n    result = []\\n    for j, i in enumerate(I[0]):\\n        label_id = int(labels[i])  # ✅ 强制转换为 int\\n        species_name = id_to_species[label_id]  # ✅ 此时保证能查到\\n        result.append((species_name, float(D[0][j])))\\n\\n    return id_to_organ[pred_organ_id], result'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这一段是再关闭脚本之后的加载训练的植物模型的代码\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ✅ 1. 加载类别映射\n",
    "with open(\"species_classes_top997.json\") as f:\n",
    "    species_classes = json.load(f)\n",
    "with open(\"organ_classes_top997.json\") as f:\n",
    "    organ_classes = json.load(f)\n",
    "\n",
    "id_to_species = {int(v): k for k, v in species_classes.items()}  # ✅ 保证key是int\n",
    "id_to_organ = {int(v): k for k, v in organ_classes.items()}\n",
    "\n",
    "# ✅ 2. 加载 organ_model\n",
    "organ_model = models.resnet18(pretrained=False)\n",
    "organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.load_state_dict(torch.load(\"organ_classifier_top997.pth\", map_location=device))\n",
    "organ_model = organ_model.to(device).eval()\n",
    "\n",
    "\n",
    "# 保存模型 state_dict\n",
    "\n",
    "# ✅ 3. 加载 CLIP 模型\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "torch.save(clip_model.state_dict(), 'ViT-L-14.pt')\n",
    "#clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained=None)\n",
    "#clip_model.load_state_dict(torch.load('ViT-L-14.pt', map_location=device))\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "# ✅ 4. 加载 organ_transform\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ✅ 5. 加载 faiss index 和 labels\n",
    "organ_index = {}\n",
    "organ_label_array = {}\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    index_path = f\"clip_index_top997/index_{organ_id}.index\"\n",
    "    label_path = f\"clip_index_top997/labels_{organ_id}.npy\"\n",
    "    if os.path.exists(index_path) and os.path.exists(label_path):\n",
    "        organ_index[int(organ_id)] = faiss.read_index(index_path)\n",
    "        organ_label_array[int(organ_id)] = np.load(label_path)\n",
    "    else:\n",
    "        print(f\"❌ 缺失器官 {organ_name} 的索引或标签数组，跳过\")\n",
    "\n",
    "\n",
    "#只有植物种类分类的部分，可不运行\n",
    "'''def predict_species(image_path, topk=5):\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 检索\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    # Step 3: 相似度检索\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "\n",
    "    result = []\n",
    "    for j, i in enumerate(I[0]):\n",
    "        label_id = int(labels[i])  # ✅ 强制转换为 int\n",
    "        species_name = id_to_species[label_id]  # ✅ 此时保证能查到\n",
    "        result.append((species_name, float(D[0][j])))\n",
    "\n",
    "    return id_to_organ[pred_organ_id], result'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "33b825d7-a7e2-4ad7-8787-8222cb233e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Top5 植物预测结果：\n",
      "  - Sonchus arvensis（相似度：0.3032）\n",
      "  - Rumex crispus（相似度：0.2862）\n",
      "  - Acer rubrum（相似度：0.2860）\n",
      "  - Malva sylvestris（相似度：0.2856）\n",
      "  - Equisetum arvense（相似度：0.2828）\n"
     ]
    }
   ],
   "source": [
    "#重新加载后的测试\n",
    "\n",
    "# ===== Step 8: 测试推理 =====\n",
    "img_path = \"/mnt/e/code/plants-classification-conda/test_bottel.png\"  # 替换为你自己的图片路径\n",
    "\n",
    "organ_name, top_results = predict_species(img_path)\n",
    "#print(\"✅ 预测器官：\", organ_name)\n",
    "print(\"✅ Top5 植物预测结果：\")\n",
    "for name, score in top_results:\n",
    "    print(f\"  - {name}（相似度：{score:.4f}）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c21a35-4b0b-4936-b7e6-44c8847ac066",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 可以模型demo\n",
    "### 使用openclip\n",
    "##### 上面organ_classifier_pth模型保留，需要预训练+加载\n",
    "##### 全流程使用plant_top997数据和相应映射，organ_classifier_pth模型已经存储，训练前需要调用\n",
    "##### 以下是功能全流程（写在一个框内）：是否是植物、植物物种是否在我的数据集中、植物物种预测、病虫害判断、、\n",
    "##### 可单张处理、可批处理（max=9）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29e1331-039a-4821-8301-5d099d30401e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def resize_and_cache(df_csv, output_path, size=224):\\n    df = pd.read_csv(df_csv)\\n    images, labels, hashes = [], [], []\\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {output_path}\"):\\n        img = cv2.imread(row[\\'image_path\\'])\\n        if img is None:\\n            continue\\n        img = cv2.resize(img, (size, size))\\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n        images.append(img)\\n        labels.append(row[\\'species\\'])\\n\\n        # 用于功能1：记录图像哈希值\\n        with open(row[\\'image_path\\'], \\'rb\\') as f:\\n            file_hash = hashlib.md5(f.read()).hexdigest()\\n            hashes.append(file_hash)\\n\\n    images = np.stack(images)\\n    labels = np.array(labels)\\n    hashes = np.array(hashes)\\n    np.savez_compressed(output_path, images=images, labels=labels, hashes=hashes)\\n    print(f\"✅ 缓存完成，共 {len(labels)} 张图像 -> {output_path}\")\\n\\nresize_and_cache(\\n    df_csv=\"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\",\\n    output_path=\"hashes_num_top997.npz\",\\n    size=224\\n)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import hashlib\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "\n",
    "# ==== Step 0: 初始化路径与模型 ====\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes_top997.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes_top997.json\"\n",
    "npz_path = \"/mnt/e/code/plants-classification-conda/hashes_num_top997.npz\"\n",
    "#test_image = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "\n",
    "'''def resize_and_cache(df_csv, output_path, size=224):\n",
    "    df = pd.read_csv(df_csv)\n",
    "    images, labels, hashes = [], [], []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {output_path}\"):\n",
    "        img = cv2.imread(row['image_path'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "        labels.append(row['species'])\n",
    "\n",
    "        # 用于功能1：记录图像哈希值\n",
    "        with open(row['image_path'], 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            hashes.append(file_hash)\n",
    "\n",
    "    images = np.stack(images)\n",
    "    labels = np.array(labels)\n",
    "    hashes = np.array(hashes)\n",
    "    np.savez_compressed(output_path, images=images, labels=labels, hashes=hashes)\n",
    "    print(f\"✅ 缓存完成，共 {len(labels)} 张图像 -> {output_path}\")\n",
    "\n",
    "resize_and_cache(\n",
    "    df_csv=\"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\",\n",
    "    output_path=\"hashes_num_top997.npz\",\n",
    "    size=224\n",
    ")'''\n",
    "\n",
    "#df_filtered=pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515cbfea-0b10-49fd-9815-a0d2d044e35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ_model = resnet18(pretrained=True)\\norgan_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\\norgan_model.load_state_dict(torch.load(\"organ_classifier_top997.pth\"))\\norgan_model.eval().cuda()\\n\\n# CLIP model\\nclip_model, _, preprocess = open_clip.create_model_and_transforms(\\'ViT-L-14\\', pretrained=\\'openai\\')\\nclip_model = clip_model.cuda().eval()\\n\\n# 加载图像哈希   不需要用\\n#known_hashes = np.load(npz_path)[\\'hashes\\']\\n\\n# ==== Step 2: 构建 CLIP 检索索引 ====\\norgan_index, organ_label_array = {}, {}\\nfor organ_name, organ_id in organ_classes.items():\\n    organ_df = df[df[\\'organ\\'] == organ_name].reset_index(drop=True)\\n    embeddings, labels = [], []\\n    for _, row in tqdm(organ_df.iterrows(), total=len(organ_df), desc=f\"Indexing {organ_name}\"):\\n        try:\\n            image = preprocess(Image.open(row[\\'image_path\\']).convert(\"RGB\")).unsqueeze(0).cuda()\\n            with torch.no_grad():\\n                emb = clip_model.encode_image(image)\\n                emb = emb / emb.norm(dim=-1, keepdim=True)\\n            embeddings.append(emb.cpu().numpy())\\n            labels.append(species_classes[row[\\'label\\']])\\n        except:\\n            continue\\n    if embeddings:\\n        emb_arr = np.vstack(embeddings).astype(\"float32\")\\n        index = faiss.IndexFlatIP(emb_arr.shape[1])\\n        index.add(emb_arr)\\n        organ_index[organ_id] = index\\n        organ_label_array[organ_id] = np.array(labels)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==== Step 1: 加载数据与模型 ====\n",
    "df = pd.read_csv(csv_path)\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "organ_classes = {str(k).strip().lower(): v for k, v in organ_classes.items()}\n",
    "\n",
    "# organ model\n",
    "'''organ_model = resnet18(pretrained=True)\n",
    "organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.load_state_dict(torch.load(\"organ_classifier_top997.pth\"))\n",
    "organ_model.eval().cuda()\n",
    "\n",
    "# CLIP model\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "clip_model = clip_model.cuda().eval()\n",
    "\n",
    "# 加载图像哈希   不需要用\n",
    "#known_hashes = np.load(npz_path)['hashes']\n",
    "\n",
    "# ==== Step 2: 构建 CLIP 检索索引 ====\n",
    "organ_index, organ_label_array = {}, {}\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    organ_df = df[df['organ'] == organ_name].reset_index(drop=True)\n",
    "    embeddings, labels = [], []\n",
    "    for _, row in tqdm(organ_df.iterrows(), total=len(organ_df), desc=f\"Indexing {organ_name}\"):\n",
    "        try:\n",
    "            image = preprocess(Image.open(row['image_path']).convert(\"RGB\")).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                emb = clip_model.encode_image(image)\n",
    "                emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(species_classes[row['label']])\n",
    "        except:\n",
    "            continue\n",
    "    if embeddings:\n",
    "        emb_arr = np.vstack(embeddings).astype(\"float32\")\n",
    "        index = faiss.IndexFlatIP(emb_arr.shape[1])\n",
    "        index.add(emb_arr)\n",
    "        organ_index[organ_id] = index\n",
    "        organ_label_array[organ_id] = np.array(labels)'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce227ec-83b7-4476-a390-625ad56d822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##============================= 功能1改进版----测试第一版 =========================####\n",
    "## ==============   正确版本----------是否植物-------是否在我的数据中？？？？？？？？？？-------识别植物 =================\n",
    "\n",
    "\n",
    "## ----------- 病虫害-----------病虫害线下大模型-----------------   for 节省token ================\n",
    "\n",
    "\n",
    "# ====== Step 4: 两阶段推理函数 ======\n",
    "\n",
    "# ====== 是否是植物======\n",
    "\n",
    "#调整---------三---\n",
    "def is_plant_clip(image_path):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    top_idx = logits.argmax().item()\n",
    "    top_label = text_labels[top_idx]\n",
    "    #print(f\"🌱 是否植物判断：Top类 = {top_label}, 得分 = {logits[top_idx]:.4f}\")\n",
    "    return top_label == \"a photo of a plant\"\n",
    "    score = logits[0].item()  # 植物标签得分\n",
    "    #print(\"🌱 是否植物得分:\", score)\n",
    "    return score >= threshold\n",
    "\n",
    "\n",
    "# ====== 是否是我数据库的植物=======\n",
    "# ====== 植物预测=======\n",
    "\n",
    "# 病虫害功能\n",
    "def is_diseased_clip_from_images_voting(image_paths, vote_threshold=0.7):\n",
    "\n",
    "    healthy_prompts = [\n",
    "        \"The leaves of healthy plants are usually bright green\",\n",
    "        \"The leaves of healthy plants are usually full and shiny, with no obvious signs of disease or insect damage on the leaf surface\",\n",
    "        \"Healthy plants usually have strong, straight stems that are able to support the weight of the plant\",\n",
    "        \"Healthy plants will show vigorous growth, including sprouting new leaves, extending branches and blooming flowers\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"A healthy plant has bright flowers with intact petals and no wilting, falling off, or diseased spots\"\n",
    "        \"The fruit of a healthy plant is full and has no cracks, rot or lesions. The fruit skin is normal color\"\n",
    "    ]\n",
    "    diseased_prompts = [\n",
    "        \"Unhealthy plant leaves or flowers will have spots or patches of different shapes, sizes and colors, such as round, oval, polygonal, wheel-shaped\",\n",
    "        \"Unhealthy plants have curled, shrunken, twisted leaves and flowers, and misshapen and stunted flowers\",\n",
    "        \"Tumor-like protrusions appear on the stem, such as rose cancer, and swelling occurs\",\n",
    "        'Soft rot, wet rot or dry rot on the stem',\n",
    "        \"Unhealthy plants may have holes, nicks, or signs of being eaten on their leaves and petals\",\n",
    "        \"Unhealthy plants may have visible insects, such as aphids and spider mites. Some pests will leave spider web-like silk\",\n",
    "        \"Leaves lose their normal green color, show yellowing symptoms, partially or completely die, and appear brown or black\",\n",
    "        \"The petals may appear water-soaked, rotten, softened, or even completely rotten.\"\n",
    "    ]\n",
    "\n",
    "    image_features_list = []\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                feat = clip_model.encode_image(image)\n",
    "                feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                image_features_list.append(feat)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 图像处理失败: {path}, 错误: {e}\")\n",
    "\n",
    "    if not image_features_list:\n",
    "        print(\"⚠️ 无有效图像特征，无法判断\")\n",
    "        return False\n",
    "\n",
    "    # 平均图像特征\n",
    "    image_features = torch.cat(image_features_list, dim=0).mean(dim=0, keepdim=True)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    diseased_votes = 0\n",
    "    total_votes = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h_prompt, d_prompt in zip(healthy_prompts, diseased_prompts):\n",
    "            text_tokens = open_clip.tokenize([h_prompt, d_prompt]).cuda()\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "            pred = logits.argmax().item()\n",
    "            total_votes += 1\n",
    "            if pred == 1:\n",
    "                diseased_votes += 1\n",
    "\n",
    "    ratio = diseased_votes / total_votes\n",
    "    #print(f\"🦠 病虫害投票结果（融合图像）: {diseased_votes}/{total_votes}（比例: {ratio:.2f}）\")\n",
    "    return ratio >= vote_threshold\n",
    "\n",
    "\n",
    "#融合功能----主函数\n",
    "\n",
    "def batch_predict_species_with_disease(image_paths, topk=5, sim_threshold=0.5, vote_threshold=0.7):\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"❌ 输入图像列表为空\")\n",
    "        return\n",
    "\n",
    "    max_batch = 9\n",
    "    image_paths = image_paths[:max_batch]\n",
    "\n",
    "    is_plant_flags = [is_plant_clip(p) for p in image_paths]\n",
    "    num_plant = sum(is_plant_flags)\n",
    "    print(f\"🌿 共检测到 {num_plant}/{len(image_paths)} 张为植物图像\")\n",
    "\n",
    "    if num_plant <= len(image_paths) // 2:\n",
    "        print(\"❌ 不是植物，无法识别\")\n",
    "        return\n",
    "\n",
    "    predicted_species = []\n",
    "    species_to_images = {}\n",
    "\n",
    "    \n",
    "#植物器官判定\n",
    "#植物种类判断\n",
    "    for i, is_plant in enumerate(is_plant_flags):\n",
    "        if not is_plant:\n",
    "            continue\n",
    "        image_path = image_paths[i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor = organ_transform(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                pred_organ_id = organ_model(img_tensor).argmax(1).item()#器官预测：判断花叶果实\n",
    "\n",
    "            clip_img = preprocess(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                # 提取图像特征（用于后续查找相似植物）\n",
    "                query_vec = clip_model.encode_image(clip_img)\n",
    "                query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "                query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "            # 使用 Faiss 搜索最相似的植物种类（对应当前器官的索引库）\n",
    "            index = organ_index[pred_organ_id]\n",
    "            labels = organ_label_array[pred_organ_id]\n",
    "            D, I = index.search(query_np, topk)\n",
    "\n",
    "            # 获取 Top1 相似植物种类标签\n",
    "            top_score = D[0][0]\n",
    "            top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "            if top_score >= sim_threshold:\n",
    "                predicted_species.append(top_label)\n",
    "                species_to_images.setdefault(top_label, []).append(image_path)\n",
    "            else:\n",
    "                predicted_species.append(\"unknown\")\n",
    "\n",
    "\n",
    "## 总体输出：for 单张 and 批量\n",
    "## 融合判定\n",
    "            # ✅ 单图模式下立即输出结果\n",
    "            if len(image_paths) == 1:\n",
    "                print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "                if top_score < sim_threshold:\n",
    "                    print(f\"⚠️ 相似度过低（最高得分 {top_score:.4f}），植物种类无法确认\")\n",
    "                    return\n",
    "                else:\n",
    "                    #print(f\"🌿 识别为植物种类: {top_label}（得分: {top_score:.4f}）\")\n",
    "\n",
    "                    if top_score >= 0.8:\n",
    "                        # ✅ 相似度高，输出 Top1\n",
    "                        print(f\"🌿 识别为植物种类: {top_label}（得分: {top_score:.4f}）\")\n",
    "                    else:\n",
    "                        # ✅ 相似度一般，输出 Top3\n",
    "                        print(f\"⚠️ 相似度较低（最高得分 {top_score:.4f}），展示 Top3 候选:\")\n",
    "                        for j, i in enumerate(I[0][:3]):\n",
    "                            species = id_to_species[labels[i]]\n",
    "                            score = float(D[0][j])\n",
    "                            print(f\"🔸 候选 {j+1}: {species}（得分: {score:.4f}）\")\n",
    "                \n",
    "                    has_disease = is_diseased_clip_from_images_voting([image_path], vote_threshold=vote_threshold)\n",
    "                    print(f\"🦠 病虫害判断: {'是' if has_disease else '否'}\")\n",
    "                    return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 图像处理失败: {image_path}, 错误: {e}\")\n",
    "            predicted_species.append(\"unknown\")\n",
    "\n",
    "    known_species = [s for s in predicted_species if s != \"unknown\"]\n",
    "    if len(image_paths) > 1 and len(known_species) <= len(predicted_species) // 2:\n",
    "        print(\"⚠️ 大多数植物图像无法识别为数据集中已知物种\")\n",
    "        return\n",
    "\n",
    "    counter = collections.Counter(known_species)\n",
    "    top3 = counter.most_common(3)\n",
    "    top1_species = top3[0][0]\n",
    "    disease_image_paths = species_to_images.get(top1_species, [])\n",
    "    has_disease = is_diseased_clip_from_images_voting(disease_image_paths, vote_threshold=vote_threshold)\n",
    "\n",
    "    print(\"🌼 可能的植物是：\")\n",
    "    for name, count in top3:\n",
    "        print(f\"✅ {name}: {count}票\")\n",
    "\n",
    "    print(f\"🦠 病虫害判断（Top1: {top1_species}）: {'是' if has_disease else '否'}（共 {len(disease_image_paths)} 张图像）\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605b11f8-ecc1-482c-b2d7-b4da201a571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: leaf\n",
      "🌿 识别为植物种类: Populus tremula（得分: 0.8713）\n",
      "🦠 病虫害判断: 否\n"
     ]
    }
   ],
   "source": [
    "#全流程测试\n",
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula2.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a44a5a4f-67a9-4af1-bf52-158f3fb4ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 共检测到 0/1 张为植物图像\n",
      "❌ 不是植物，无法识别\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_keyboard.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37137aa1-b176-4d62-9216-b23a9d2e31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: flower\n",
      "🌿 识别为植物种类: Vitex agnus-castus（得分: 0.8675）\n",
      "🦠 病虫害判断: 否\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebc77fe0-cc46-443a-b373-312a6f124e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 共检测到 4/5 张为植物图像\n",
      "🌼 可能的植物是：\n",
      "✅ Populus tremula: 3票\n",
      "✅ Rumex crispus: 1票\n",
      "🦠 病虫害判断（Top1: Populus tremula）: 否（共 3 张图像）\n"
     ]
    }
   ],
   "source": [
    "# 示例：支持 1~9 张图片\n",
    "batch_predict_species_with_disease([\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula2.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_car.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_Rumex_crispus.jpg\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e88410-6e40-4de3-9bc2-b4196226b372",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41f33efc-9ed6-4276-b6fa-c5c176621c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已生成兼容变量导出式 model_loader.py，可以直接和 inference/app 配套使用。\n"
     ]
    }
   ],
   "source": [
    "model_loader_code = '''\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import open_clip\n",
    "import faiss\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "\n",
    "WEIGHTS_DIR = \"weights\"\n",
    "INDEX_DIR = os.path.join(WEIGHTS_DIR, \"clip_index_top997\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. 加载 organ_classes & species_classes\n",
    "with open(os.path.join(WEIGHTS_DIR, \"organ_classes_top997.json\")) as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(os.path.join(WEIGHTS_DIR, \"species_classes_top997.json\")) as f:\n",
    "    species_classes = json.load(f)\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "\n",
    "# 2. Organ 分类模型\n",
    "organ_model = models.resnet18(pretrained=False)\n",
    "organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.load_state_dict(torch.load(os.path.join(WEIGHTS_DIR, \"organ_classifier_top997.pth\"), map_location=device))\n",
    "organ_model = organ_model.to(device).eval()\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3. OpenCLIP\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-L-14\", pretrained=None)\n",
    "clip_model.load_state_dict(torch.load('weights/ViT-L-14.pt', map_location=device))\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "# 4. FAISS 向量索引和标签\n",
    "organ_index = {}\n",
    "organ_label_array = {}\n",
    "for name, organ_id in organ_classes.items():\n",
    "    index_path = os.path.join(INDEX_DIR, f\"index_{organ_id}.index\")\n",
    "    label_path = os.path.join(INDEX_DIR, f\"labels_{organ_id}.npy\")\n",
    "    if os.path.exists(index_path) and os.path.exists(label_path):\n",
    "        organ_index[int(organ_id)] = faiss.read_index(index_path)\n",
    "        organ_label_array[int(organ_id)] = np.load(label_path)\n",
    "'''\n",
    "\n",
    "with open(\"plant_api/model_loader.py\", \"w\") as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "print(\"✅ 已生成兼容变量导出式 model_loader.py，可以直接和 inference/app 配套使用。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a650db2-081e-449d-9474-bd2484563b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已生成完整 inference.py\n"
     ]
    }
   ],
   "source": [
    "inference_py_content = '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import collections\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from model_loader import (\n",
    "    organ_model,\n",
    "    organ_transform,\n",
    "    clip_model,\n",
    "    preprocess,\n",
    "    organ_index,\n",
    "    organ_label_array,\n",
    "    id_to_organ,\n",
    "    id_to_species\n",
    ")\n",
    "\n",
    "\n",
    "def download_image(url, temp_dir=\"uploads\"):\n",
    "    \"\"\"下载网络图片并保存到临时文件\"\"\"\n",
    "    try:\n",
    "        print(f\"🌐 正在下载: {url}\")\n",
    "        \n",
    "        # 添加User-Agent避免被某些服务器拒绝\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(\n",
    "            url, \n",
    "            timeout=30, \n",
    "            headers=headers, \n",
    "            allow_redirects=True,\n",
    "            verify=True,  # SSL验证\n",
    "            stream=True   # 流式下载，避免大文件占用内存\n",
    "        )\n",
    "        \n",
    "        # 检查HTTP状态码\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ HTTP错误 {response.status_code}: {url}\")\n",
    "            if response.status_code == 404:\n",
    "                print(f\"   文件不存在: {url}\")\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"   访问被拒绝: {url}\")\n",
    "            elif response.status_code == 500:\n",
    "                print(f\"   服务器内部错误: {url}\")\n",
    "            return None\n",
    "        \n",
    "        # 检查Content-Type\n",
    "        content_type = response.headers.get('content-type', '').lower()\n",
    "        if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'gif', 'bmp']):\n",
    "            print(f\"❌ 非图片类型: {content_type}, URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        # 检查内容长度\n",
    "        content_length = len(response.content)\n",
    "        if content_length < 100:  # 太小可能是错误页面\n",
    "            print(f\"❌ 文件太小 ({content_length} bytes): {url}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"✅ 下载成功 ({content_length} bytes)\")\n",
    "        \n",
    "        # 创建临时目录\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        # 生成临时文件名\n",
    "        file_ext = url.split('.')[-1].split('?')[0].lower()  # 处理URL参数\n",
    "        if file_ext not in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:\n",
    "            file_ext = 'jpg'  # 默认扩展名\n",
    "        \n",
    "        temp_filename = f\"{uuid.uuid4()}.{file_ext}\"\n",
    "        temp_path = os.path.join(temp_dir, temp_filename)\n",
    "        \n",
    "        # 验证是否为有效图片并保存\n",
    "        try:\n",
    "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            img.save(temp_path)\n",
    "            print(f\"💾 图片已保存: {temp_path}\")\n",
    "            return temp_path\n",
    "        except Exception as img_error:\n",
    "            print(f\"❌ 图片格式错误: {img_error}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.SSLError as ssl_error:\n",
    "        print(f\"⚠️  SSL错误，尝试关闭SSL验证: {url}\")\n",
    "        try:\n",
    "            # 尝试关闭SSL验证重新下载\n",
    "            response = requests.get(\n",
    "                url, \n",
    "                timeout=30, \n",
    "                headers=headers, \n",
    "                allow_redirects=True,\n",
    "                verify=False,  # 关闭SSL验证\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"✅ SSL降级成功\")\n",
    "                content_length = len(response.content)\n",
    "                if content_length < 100:\n",
    "                    print(f\"❌ 文件太小 ({content_length} bytes): {url}\")\n",
    "                    return None\n",
    "                \n",
    "                # 创建临时目录\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                \n",
    "                # 生成临时文件名\n",
    "                file_ext = url.split('.')[-1].split('?')[0].lower()\n",
    "                if file_ext not in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:\n",
    "                    file_ext = 'jpg'\n",
    "                \n",
    "                temp_filename = f\"{uuid.uuid4()}.{file_ext}\"\n",
    "                temp_path = os.path.join(temp_dir, temp_filename)\n",
    "                \n",
    "                # 验证并保存图片\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                    img.save(temp_path)\n",
    "                    print(f\"💾 图片已保存: {temp_path}\")\n",
    "                    return temp_path\n",
    "                except Exception as img_error:\n",
    "                    print(f\"❌ 图片格式错误: {img_error}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"❌ SSL降级后仍失败: {response.status_code}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as retry_error:\n",
    "            print(f\"❌ SSL降级失败: {retry_error}\")\n",
    "            return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"❌ 下载超时: {url}\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"❌ 连接错误: {url}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ 请求错误: {e}, URL: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 未知错误: {e}, URL: {url}\")\n",
    "        return None\n",
    "\n",
    "def download_images_from_urls(urls, save_dir):\n",
    "    downloaded = []\n",
    "    failed = []\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "    for url in urls:\n",
    "        try:\n",
    "            file_ext = url.split(\"?\")[0].split(\".\")[-1]\n",
    "            filename = f\"{uuid.uuid4()}.{file_ext}\"\n",
    "            save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "            r = requests.get(url, headers=headers, timeout=10, proxies={\"http\": None, \"https\": None})\n",
    "            if r.status_code == 200:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                downloaded.append(save_path)\n",
    "            else:\n",
    "                print(f\"状态码异常：{r.status_code}\")\n",
    "                failed.append(url)\n",
    "        except Exception as e:\n",
    "            print(f\"下载失败：{url}, 错误：{e}\")\n",
    "            failed.append(url)\n",
    "    return downloaded, failed\n",
    "\n",
    "\n",
    "def is_plant_clip(image_path):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(next(clip_model.parameters()).device)\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(next(clip_model.parameters()).device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    top_idx = logits.argmax().item()\n",
    "    top_label = text_labels[top_idx]\n",
    "    return top_label == \"a photo of a plant\"\n",
    "\n",
    "def is_diseased_clip_from_images_voting(image_paths, vote_threshold=0.7):\n",
    "    healthy_prompts = [\n",
    "        \"The leaves of healthy plants are usually bright green\",\n",
    "        \"The leaves of healthy plants are usually full and shiny, with no obvious signs of disease or insect damage on the leaf surface\",\n",
    "        \"Healthy plants usually have strong, straight stems that are able to support the weight of the plant\",\n",
    "        \"Healthy plants will show vigorous growth, including sprouting new leaves, extending branches and blooming flowers\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"A healthy plant has bright flowers with intact petals and no wilting, falling off, or diseased spots\",\n",
    "        \"The fruit of a healthy plant is full and has no cracks, rot or lesions. The fruit skin is normal color\"\n",
    "    ]\n",
    "    diseased_prompts = [\n",
    "        \"Unhealthy plant leaves or flowers will have spots or patches of different shapes, sizes and colors, such as round, oval, polygonal, wheel-shaped\",\n",
    "        \"Unhealthy plants have curled, shrunken, twisted leaves and flowers, and misshapen and stunted flowers\",\n",
    "        \"Tumor-like protrusions appear on the stem, such as rose cancer, and swelling occurs\",\n",
    "        'Soft rot, wet rot or dry rot on the stem',\n",
    "        \"Unhealthy plants may have holes, nicks, or signs of being eaten on their leaves and petals\",\n",
    "        \"Unhealthy plants may have visible insects, such as aphids and spider mites. Some pests will leave spider web-like silk\",\n",
    "        \"Leaves lose their normal green color, show yellowing symptoms, partially or completely die, and appear brown or black\",\n",
    "        \"The petals may appear water-soaked, rotten, softened, or even completely rotten.\"\n",
    "    ]\n",
    "\n",
    "    image_features_list = []\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(next(clip_model.parameters()).device)\n",
    "            with torch.no_grad():\n",
    "                feat = clip_model.encode_image(image)\n",
    "                feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                image_features_list.append(feat)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 图像处理失败: {path}, 错误: {e}\")\n",
    "\n",
    "    if not image_features_list:\n",
    "        print(\"⚠️ 无有效图像特征，无法判断\")\n",
    "        return False\n",
    "\n",
    "    image_features = torch.cat(image_features_list, dim=0).mean(dim=0, keepdim=True)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    diseased_votes = 0\n",
    "    total_votes = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h_prompt, d_prompt in zip(healthy_prompts, diseased_prompts):\n",
    "            text_tokens = open_clip.tokenize([h_prompt, d_prompt]).to(next(clip_model.parameters()).device)\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "            pred = logits.argmax().item()\n",
    "            total_votes += 1\n",
    "            if pred == 1:\n",
    "                diseased_votes += 1\n",
    "\n",
    "    ratio = diseased_votes / total_votes\n",
    "    return bool(ratio >= vote_threshold)\n",
    "\n",
    "def batch_predict_species_with_disease(image_paths, topk=3, sim_threshold=0.5, vote_threshold=0.7):\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    if not image_paths:\n",
    "        return {\n",
    "            \"code\": 400,\n",
    "            \"msg\": \"错误：输入图像列表为空\",\n",
    "            \"data\": {}\n",
    "        }\n",
    "\n",
    "    max_batch = 3\n",
    "    image_paths = image_paths[:max_batch]\n",
    "\n",
    "    is_plant_flags = [is_plant_clip(p) for p in image_paths]\n",
    "    num_plant = sum(is_plant_flags)\n",
    "\n",
    "    if num_plant <= len(image_paths) // 2:\n",
    "        return {\n",
    "            \"code\": 200,\n",
    "            \"msg\": \"成功\",\n",
    "            \"data\": {\n",
    "                \"is_plant\": False,\n",
    "                \"is_database\": False,\n",
    "                \"plants\": [],\n",
    "                \"is_infected\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "    predicted_species = []\n",
    "    species_to_images = {}\n",
    "    species_to_scores = {}  # 新增：存储每个物种的相似度分数\n",
    "    organ_results = []\n",
    "\n",
    "    for i, is_plant in enumerate(is_plant_flags):\n",
    "        if not is_plant:\n",
    "            continue\n",
    "        image_path = image_paths[i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor = organ_transform(image).unsqueeze(0).to(next(organ_model.parameters()).device)\n",
    "            with torch.no_grad():\n",
    "                pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "            clip_img = preprocess(image).unsqueeze(0).to(next(clip_model.parameters()).device)\n",
    "            with torch.no_grad():\n",
    "                query_vec = clip_model.encode_image(clip_img)\n",
    "                query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "                query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "            index = organ_index[pred_organ_id]\n",
    "            labels = organ_label_array[pred_organ_id]\n",
    "            D, I = index.search(query_np, topk)\n",
    "\n",
    "            top_score = D[0][0]\n",
    "            top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "            if top_score >= sim_threshold:\n",
    "                predicted_species.append(top_label)\n",
    "                species_to_images.setdefault(top_label, []).append(image_path)\n",
    "                # 存储相似度分数\n",
    "                if top_label not in species_to_scores:\n",
    "                    species_to_scores[top_label] = []\n",
    "                species_to_scores[top_label].append(float(top_score))\n",
    "            else:\n",
    "                predicted_species.append(\"unknown\")\n",
    "\n",
    "            organ_results.append(id_to_organ[pred_organ_id])\n",
    "\n",
    "            if len(image_paths) == 1:\n",
    "                if top_score < sim_threshold:\n",
    "                    return {\n",
    "                        \"code\": 200,\n",
    "                        \"msg\": \"成功\",\n",
    "                        \"data\": {\n",
    "                            \"is_plant\": True,\n",
    "                            \"is_database\": False,\n",
    "                            \"plants\": [],\n",
    "                            \"is_infected\": False\n",
    "                        }\n",
    "                    }\n",
    "                else:\n",
    "                    candidates = []\n",
    "                    for j, idx in enumerate(I[0][:3]):\n",
    "                        species = id_to_species[labels[idx]]\n",
    "                        score = float(D[0][j])\n",
    "                        candidates.append({\n",
    "                            \"latin_name\": species,\n",
    "                            \"confidence\": round(score, 4)\n",
    "                        })\n",
    "                    is_infected = is_diseased_clip_from_images_voting([image_path], vote_threshold=vote_threshold)\n",
    "                    return {\n",
    "                        \"code\": 200,\n",
    "                        \"msg\": \"成功\",\n",
    "                        \"data\": {\n",
    "                            \"is_plant\": True,\n",
    "                            \"is_database\": bool(top_score >= sim_threshold),\n",
    "                            \"plants\": candidates,\n",
    "                            \"is_infected\": bool(is_infected)\n",
    "                        }\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            predicted_species.append(\"unknown\")\n",
    "\n",
    "    known_species = [s for s in predicted_species if s != \"unknown\"]\n",
    "    if len(image_paths) > 1 and len(known_species) <= len(predicted_species) // 2:\n",
    "        return {\n",
    "            \"code\": 200,\n",
    "            \"msg\": \"成功\",\n",
    "            \"data\": {\n",
    "                \"is_plant\": True,\n",
    "                \"is_database\": False,\n",
    "                \"plants\": [],\n",
    "                \"is_infected\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "    counter = collections.Counter(known_species)\n",
    "    top3 = counter.most_common(3)\n",
    "    top1_species = top3[0][0]\n",
    "    disease_image_paths = species_to_images.get(top1_species, [])\n",
    "    is_infected = is_diseased_clip_from_images_voting(disease_image_paths, vote_threshold=vote_threshold)\n",
    "\n",
    "    # 构建plants列表，包含top3结果\n",
    "    plants_list = []\n",
    "    for name, votes in top3:\n",
    "        # 使用实际相似度分数的平均值作为confidence\n",
    "        if name in species_to_scores and species_to_scores[name]:\n",
    "            avg_confidence = sum(species_to_scores[name]) / len(species_to_scores[name])\n",
    "            confidence = round(avg_confidence, 4)\n",
    "        else:\n",
    "            # 如果没有分数记录，使用基于投票的估算\n",
    "            confidence = round(votes / len(predicted_species) * 0.8, 4)\n",
    "        \n",
    "        plants_list.append({\n",
    "            \"latin_name\": name,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"code\": 200,\n",
    "        \"msg\": \"成功\",\n",
    "        \"data\": {\n",
    "            \"is_plant\": True,\n",
    "            \"is_database\": top1_species != \"unknown\",\n",
    "            \"plants\": plants_list,\n",
    "            \"is_infected\": bool(is_infected)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def predict_plant(image_path):\n",
    "    return batch_predict_species_with_disease([image_path])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "with open(\"plant_api/inference.py\", \"w\") as f:\n",
    "    f.write(inference_py_content)\n",
    "\n",
    "print(\"✅ 已生成完整 inference.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19dec8b1-2193-4440-8e51-41e13790b98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ app.py 文件已生成，适配推理接口'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_py_content = '''\n",
    "\n",
    "from fastapi import FastAPI, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "from inference import predict_plant, batch_predict_species_with_disease, download_image\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"植物识别API\",\n",
    "    description=\"支持远程图片URL的植物识别服务，提供Form和JSON两种输入方式\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "UPLOAD_DIR = \"uploads\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Pydantic模型定义 - JSON格式\n",
    "class URLRequest(BaseModel):\n",
    "    image_urls: List[str] = Field(\n",
    "        ..., \n",
    "        min_items=1, \n",
    "        max_items=10,\n",
    "        description=\"图片URL列表，最多10个\",\n",
    "        example=[\n",
    "            \"https://httpbin.org/image/jpeg\",\n",
    "            \"https://httpbin.org/image/png\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict_from_urls_json(request: URLRequest):\n",
    "    \"\"\"\n",
    "    通过远程图片URL进行植物识别 (JSON格式)\n",
    "    \n",
    "    支持：\n",
    "    - 单个或多个URL（最多10个）\n",
    "    - JSON格式输入\n",
    "    - 自动下载图片并进行识别\n",
    "    - 返回植物种类、置信度等信息\n",
    "    \"\"\"\n",
    "    image_urls = request.image_urls\n",
    "    \n",
    "    downloaded_paths = []\n",
    "    failed_urls = []\n",
    "    \n",
    "    try:\n",
    "        # 下载图片\n",
    "        for url in image_urls:\n",
    "            result = download_image(url, UPLOAD_DIR)\n",
    "            if result:\n",
    "                downloaded_paths.append(result)\n",
    "            else:\n",
    "                failed_urls.append(url)\n",
    "        \n",
    "        if not downloaded_paths:\n",
    "            return JSONResponse(\n",
    "                content={\n",
    "                    \"code\": 400,\n",
    "                    \"msg\": f\"错误：所有图片下载失败。失败的URL: {failed_urls}\",\n",
    "                    \"data\": {}\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 进行预测\n",
    "        if len(downloaded_paths) == 1:\n",
    "            result = predict_plant(downloaded_paths[0])\n",
    "        else:\n",
    "            result = batch_predict_species_with_disease(downloaded_paths)\n",
    "        \n",
    "        # 在结果中添加下载信息\n",
    "        if failed_urls:\n",
    "            if \"msg\" in result:\n",
    "                result[\"msg\"] += f\"（注意：{len(failed_urls)}个URL下载失败）\"\n",
    "        \n",
    "        # 清理临时文件\n",
    "        for file_path in downloaded_paths:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                \n",
    "        return JSONResponse(content=result)\n",
    "\n",
    "    except Exception as e:\n",
    "        # 清理临时文件\n",
    "        for file_path in downloaded_paths:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        return JSONResponse(\n",
    "            status_code=500, \n",
    "            content={\n",
    "                \"code\": 500,\n",
    "                \"msg\": f\"服务器错误：{str(e)}\",\n",
    "                \"data\": {}\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "@app.post(\"/predict_form/\")\n",
    "async def predict_from_urls_form(image_urls: List[str] = Form(..., description=\"图片URL列表，每行一个URL，最多10个\")):\n",
    "    \"\"\"\n",
    "    通过远程图片URL进行植物识别 (Form格式，SwaggerUI友好)\n",
    "    \n",
    "    支持：\n",
    "    - 单个或多个URL（最多10个）  \n",
    "    - Form表单格式输入\n",
    "    - 在SwaggerUI中显示为表单字段\n",
    "    - 自动下载图片并进行识别\n",
    "    - 返回植物种类、置信度等信息\n",
    "    \"\"\"\n",
    "    \n",
    "    # 处理可能的多种输入格式\n",
    "    processed_urls = []\n",
    "    for url_item in image_urls:\n",
    "        # 处理可能包含多个URL的字符串（逗号分隔）\n",
    "        if ',' in url_item:\n",
    "            urls = [u.strip() for u in url_item.split(',') if u.strip()]\n",
    "            processed_urls.extend(urls)\n",
    "        # 处理换行符分隔的URL\n",
    "        elif '\\\\n' in url_item:\n",
    "            urls = [u.strip() for u in url_item.split('\\\\n') if u.strip()]\n",
    "            processed_urls.extend(urls)\n",
    "        else:\n",
    "            processed_urls.append(url_item.strip())\n",
    "    \n",
    "    # 去重并限制数量\n",
    "    processed_urls = list(dict.fromkeys(processed_urls))  # 去重保持顺序\n",
    "    if len(processed_urls) > 10:\n",
    "        return JSONResponse(\n",
    "            content={\n",
    "                \"code\": 400,\n",
    "                \"msg\": \"错误：图片URL数量不能超过10个\",\n",
    "                \"data\": {}\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    if not processed_urls:\n",
    "        return JSONResponse(\n",
    "            content={\n",
    "                \"code\": 400,\n",
    "                \"msg\": \"错误：未提供任何有效的图片URL\",\n",
    "                \"data\": {}\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    downloaded_paths = []\n",
    "    failed_urls = []\n",
    "    \n",
    "    try:\n",
    "        # 下载图片\n",
    "        for url in processed_urls:\n",
    "            result = download_image(url, UPLOAD_DIR)\n",
    "            if result:\n",
    "                downloaded_paths.append(result)\n",
    "            else:\n",
    "                failed_urls.append(url)\n",
    "        \n",
    "        if not downloaded_paths:\n",
    "            return JSONResponse(\n",
    "                content={\n",
    "                    \"code\": 400,\n",
    "                    \"msg\": f\"错误：所有图片下载失败。失败的URL: {failed_urls}\",\n",
    "                    \"data\": {}\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 进行预测\n",
    "        if len(downloaded_paths) == 1:\n",
    "            result = predict_plant(downloaded_paths[0])\n",
    "        else:\n",
    "            result = batch_predict_species_with_disease(downloaded_paths)\n",
    "        \n",
    "        # 在结果中添加下载信息\n",
    "        if failed_urls:\n",
    "            if \"msg\" in result:\n",
    "                result[\"msg\"] += f\"（注意：{len(failed_urls)}个URL下载失败）\"\n",
    "        \n",
    "        # 清理临时文件\n",
    "        for file_path in downloaded_paths:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                \n",
    "        return JSONResponse(content=result)\n",
    "\n",
    "    except Exception as e:\n",
    "        # 清理临时文件\n",
    "        for file_path in downloaded_paths:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        return JSONResponse(\n",
    "            status_code=500, \n",
    "            content={\n",
    "                \"code\": 500,\n",
    "                \"msg\": f\"服务器错误：{str(e)}\",\n",
    "                \"data\": {}\n",
    "            }\n",
    "        )\n",
    "\n",
    "'''\n",
    "\n",
    "# Write app.py to the plant_api directory\n",
    "with open(\"plant_api/app.py\", \"w\") as f:\n",
    "    f.write(app_py_content)\n",
    "\n",
    "\"✅ app.py 文件已生成，适配推理接口\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2443ca97-1a45-42f3-9bd8-1b15f6936c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ requirements.txt 文件已成功生成'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成 requirements.txt 内容\n",
    "requirements_txt_content = '''\n",
    "fastapi==0.110.0\n",
    "uvicorn[standard]==0.29.0\n",
    "torch==2.1.0\n",
    "torchvision==0.16.0\n",
    "open_clip_torch==2.23.0\n",
    "faiss-cpu==1.7.4\n",
    "pillow==10.2.0\n",
    "numpy==1.26.4\n",
    "tqdm==4.66.2\n",
    "python-multipart\n",
    "'''\n",
    "\n",
    "# 写入 plant_api/requirements.txt\n",
    "with open(\"plant_api/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_txt_content)\n",
    "\n",
    "\"✅ requirements.txt 文件已成功生成\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19cdddd5-31ba-4075-a0e4-74eb0a37644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dockerfile 合并指令已写入。请重新 docker build -t plant-api .\n"
     ]
    }
   ],
   "source": [
    "dockerfile_content = '''\n",
    "FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n",
    "\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "\n",
    "ENV HTTP_PROXY=http://192.168.5.100:7890\n",
    "ENV HTTPS_PROXY=http://192.168.5.100:7890\n",
    "ENV http_proxy=http://192.168.5.100:7890\n",
    "ENV https_proxy=http://192.168.5.100:7890\n",
    "ENV NO_PROXY=localhost,127.0.0.1\n",
    "ENV no_proxy=localhost,127.0.0.1\n",
    "\n",
    "\n",
    "RUN pip install --upgrade pip && pip install -r requirements.txt\n",
    "\n",
    "RUN mkdir -p uploads\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n",
    "\n",
    "'''\n",
    "\n",
    "with open(\"plant_api/Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"✅ Dockerfile 合并指令已写入。请重新 docker build -t plant-api .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934ee72-041b-4e91-8f35-009bac2ed565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b9cf4-7889-4486-b2c7-9d3e4681d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3331eb-2fbc-41f2-a706-f91103239344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e5323-76af-49f6-9494-e211d078e4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb224ea8-1d6c-4cbb-9439-763a9899f452",
   "metadata": {},
   "source": [
    "# 各种模型的测试与训练方案（不可用）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa79922-3a39-4235-a913-10cfb3e2666e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 增加功能的调试过程\n",
    "#### 识别是否在我的数据库中\n",
    "#### 是否是植物\n",
    "#### 病虫害检测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7be4d79-2f5f-4d8a-bec5-bf17f41b6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载数据映射与 DataFrame\n",
    "df_filtered = pd.read_csv(\"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\")\n",
    "\n",
    "with open(\"/mnt/e/code/plants-classification-conda/organ_classes_top997.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(\"/mnt/e/code/plants-classification-conda/species_classes_top997.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "\n",
    "# 加载缓存图像\n",
    "def load_npz_cache(path):\n",
    "    data = np.load(path)\n",
    "    return data['images'], data['labels']\n",
    "\n",
    "\n",
    "#train_images, train_labels = load_npz_cache(\"train_cache.npz\")\n",
    "#val_images, val_labels = load_npz_cache(\"val_cache.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dccd51cf-f5a9-47df-b8ff-15f426170b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class CachedPlantDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249f2408-519e-4b07-9196-602435ce6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CachedPlantDataset(train_images, train_labels)\n",
    "val_dataset = CachedPlantDataset(val_images, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e9f0a0-fb67-49b1-9075-a65064510852",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_path_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 42\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m'''if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    parser = argparse.ArgumentParser()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    parser.add_argument(\"--csv\", type=str, required=True, help=\"CSV file with columns: image_path,species\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    resize_and_cache(args.csv, args.output, args.size)'''\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m resize_and_cache(csv_path, \u001b[43moutput_path_train\u001b[49m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# infer.py（整合Organ+CLIP+功能1+功能2+是否为植物识别）\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_path_train' is not defined"
     ]
    }
   ],
   "source": [
    "###！！！！！！！！！！！！！完全不可用！！！！！！\n",
    "\n",
    "\n",
    "# prepare_cache.py\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import hashlib\n",
    "\n",
    "def resize_and_cache(df_csv, output_path, size=224):\n",
    "    df = pd.read_csv(df_csv)\n",
    "    images, labels, hashes = [], [], []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {output_path}\"):\n",
    "        img = cv2.imread(row['image_path'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "        labels.append(row['species'])\n",
    "\n",
    "        # 用于功能1：记录图像哈希值\n",
    "        with open(row['image_path'], 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            hashes.append(file_hash)\n",
    "\n",
    "    images = np.stack(images)\n",
    "    labels = np.array(labels)\n",
    "    hashes = np.array(hashes)\n",
    "    np.savez_compressed(output_path, images=images, labels=labels, hashes=hashes)\n",
    "    print(f\"✅ Saved to {output_path}. Total: {len(labels)} samples\")\n",
    "\n",
    "'''if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--csv\", type=str, required=True, help=\"CSV file with columns: image_path,species\")\n",
    "    parser.add_argument(\"--output\", type=str, required=True, help=\"Output .npz filename\")\n",
    "    parser.add_argument(\"--size\", type=int, default=224, help=\"Resize image to (size, size)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    resize_and_cache(args.csv, args.output, args.size)'''\n",
    "\n",
    "resize_and_cache(csv_path, output_path_train, size=224)\n",
    "\n",
    "# infer.py（整合Organ+CLIP+功能1+功能2+是否为植物识别）\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import hashlib\n",
    "import requests\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "def load_clip_and_index(csv_path, organ_json, species_json):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    with open(organ_json, 'r') as f:\n",
    "        organ_classes = json.load(f)\n",
    "    organ_classes = {str(k).strip().lower(): v for k, v in organ_classes.items()}\n",
    "    with open(species_json, 'r') as f:\n",
    "        species_classes = json.load(f)\n",
    "    id_to_species = {v: k for k, v in species_classes.items()}\n",
    "    id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "\n",
    "    organ_model = resnet18(pretrained=True)\n",
    "    organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "    organ_model.load_state_dict(torch.load(\"organ_model.pth\"))\n",
    "    organ_model.eval().cuda()\n",
    "\n",
    "    clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "    clip_model = clip_model.cuda().eval()\n",
    "\n",
    "    organ_index, organ_label_array = {}, {}\n",
    "    for organ_name, organ_id in organ_classes.items():\n",
    "        organ_df = df[df['organ'] == organ_name].reset_index(drop=True)\n",
    "        embeddings, labels = [], []\n",
    "        for _, row in organ_df.iterrows():\n",
    "            try:\n",
    "                image = preprocess(Image.open(row['image_path']).convert(\"RGB\")).unsqueeze(0).cuda(non_blocking=True)\n",
    "                with torch.no_grad():\n",
    "                    emb = clip_model.encode_image(image)\n",
    "                    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "                labels.append(species_classes[row['label']])\n",
    "            except:\n",
    "                continue\n",
    "        if embeddings:\n",
    "            emb_arr = np.vstack(embeddings).astype(\"float32\")\n",
    "            index = faiss.IndexFlatIP(emb_arr.shape[1])\n",
    "            index.add(emb_arr)\n",
    "            organ_index[organ_id] = index\n",
    "            organ_label_array[organ_id] = np.array(labels)\n",
    "\n",
    "    return organ_model, clip_model, preprocess, organ_index, organ_label_array, organ_classes, id_to_organ, id_to_species\n",
    "\n",
    "def is_plant_image(image_path, threshold=0.3):\n",
    "    api_url = \"https://api-inference.huggingface.co/models/google/vit-base-patch16-224\"\n",
    "    headers = {\"Authorization\": \"Bearer hf_your_token_here\"}\n",
    "    with open(image_path, 'rb') as f:\n",
    "        img_bytes = f.read()\n",
    "    response = requests.post(api_url, headers=headers, files={\"file\": img_bytes})\n",
    "    try:\n",
    "        result = response.json()\n",
    "        for item in result:\n",
    "            if 'plant' in item['label'].lower() and item['score'] >= threshold:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def predict_species(image_path, known_hashes, organ_model, clip_model, preprocess, organ_index, organ_label_array, organ_classes, id_to_organ, id_to_species):\n",
    "    # 是否为植物判断\n",
    "    if not is_plant_image(image_path):\n",
    "        print(\"❌ 不是植物，请重新上传\")\n",
    "        return None\n",
    "\n",
    "    # 功能1：是否在数据库\n",
    "    with open(image_path, 'rb') as f:\n",
    "        img_bytes = f.read()\n",
    "        img_hash = hashlib.md5(img_bytes).hexdigest()\n",
    "    if img_hash not in known_hashes:\n",
    "        print(\"⚠️ 图像未在数据库中，无法识别\")\n",
    "        return None\n",
    "\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    img_tensor = transform(image).unsqueeze(0).cuda(non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 向量检索\n",
    "    clip_img = preprocess(image).unsqueeze(0).cuda(non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, 5)\n",
    "    result = [(id_to_species[labels[i]], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "    return id_to_organ[pred_organ_id], result, img_bytes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--image\", required=True, help=\"Path to image\")\n",
    "    parser.add_argument(\"--csv\", required=True)\n",
    "    parser.add_argument(\"--npz\", required=True)\n",
    "    parser.add_argument(\"--organ-json\", required=True)\n",
    "    parser.add_argument(\"--species-json\", required=True)\n",
    "    parser.add_argument(\"--disease-check\", action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    npz_data = np.load(args.npz)\n",
    "    known_hashes = npz_data['hashes']\n",
    "\n",
    "    organ_model, clip_model, preprocess, organ_index, organ_label_array, organ_classes, id_to_organ, id_to_species = load_clip_and_index(\n",
    "        args.csv, args.organ_json, args.species_json\n",
    "    )\n",
    "\n",
    "    result = predict_species(\n",
    "        args.image, known_hashes, organ_model, clip_model, preprocess,\n",
    "        organ_index, organ_label_array, organ_classes, id_to_organ, id_to_species\n",
    "    )\n",
    "\n",
    "    if result:\n",
    "        organ, preds, img_bytes = result\n",
    "        print(\"🔍 器官类别:\", organ)\n",
    "        for name, score in preds:\n",
    "            print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "        # 功能2：病虫害识别\n",
    "        '''if args.disease_check:\n",
    "            print(\"🌿 正在识别病虫害...\")\n",
    "            headers = {\"Authorization\": \"Bearer hf_your_token_here\"}  # 替换为真实 Token\n",
    "            api_url = \"https://api-inference.huggingface.co/models/nateraw/resnet50-plant-disease\"\n",
    "            response = requests.post(api_url, headers=headers, files={\"file\": img_bytes})\n",
    "            try:\n",
    "                result = response.json()\n",
    "                if isinstance(result, list):\n",
    "                    top = result[0]\n",
    "                    print(f\"🧬 病虫害诊断：{top['label']}，置信度：{top['score']:.4f}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ 模型返回异常：{result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 请求失败：{e}\")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73627a96-d35b-47cb-b6fa-a10e52fc368d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 上文中需要的数据加载部分\n",
    "#### 可能resnet50模型也使用在部分数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b982e4b8-3b9c-407f-921e-25e12158e23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hashes_num_top997.npz: 100%|██████████████████████████████| 24944/24944 [01:57<00:00, 211.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 缓存完成，共 24944 张图像 -> hashes_num_top997.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import hashlib\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==== Step 0: 初始化路径与模型 ====\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes_top997.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes_top997.json\"\n",
    "npz_path = \"/mnt/e/code/plants-classification-conda/hashes_num_top997.npz\"\n",
    "#test_image = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "\n",
    "def resize_and_cache(df_csv, output_path, size=224):\n",
    "    df = pd.read_csv(df_csv)\n",
    "    images, labels, hashes = [], [], []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {output_path}\"):\n",
    "        img = cv2.imread(row['image_path'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "        labels.append(row['species'])\n",
    "\n",
    "        # 用于功能1：记录图像哈希值\n",
    "        with open(row['image_path'], 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            hashes.append(file_hash)\n",
    "\n",
    "    images = np.stack(images)\n",
    "    labels = np.array(labels)\n",
    "    hashes = np.array(hashes)\n",
    "    np.savez_compressed(output_path, images=images, labels=labels, hashes=hashes)\n",
    "    print(f\"✅ 缓存完成，共 {len(labels)} 张图像 -> {output_path}\")\n",
    "\n",
    "resize_and_cache(\n",
    "    df_csv=\"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\",\n",
    "    output_path=\"hashes_num_top997.npz\",\n",
    "    size=224\n",
    ")\n",
    "\n",
    "#df_filtered=pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d87b041-5ca9-4357-8d6b-59ae4caef797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_13515/3255806944.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  organ_model.load_state_dict(torch.load(\"organ_classifier_top997.pth\"))\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "Indexing bark: 100%|████████████████████████████████████████████████████| 2499/2499 [00:55<00:00, 44.91it/s]\n",
      "Indexing flower: 100%|██████████████████████████████████████████████████| 9974/9974 [03:45<00:00, 44.20it/s]\n",
      "Indexing fruit: 100%|███████████████████████████████████████████████████| 2498/2498 [00:56<00:00, 44.11it/s]\n",
      "Indexing leaf: 100%|████████████████████████████████████████████████████| 9973/9973 [03:46<00:00, 44.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Step 1: 加载数据与模型 ====\n",
    "df = pd.read_csv(csv_path)\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "organ_classes = {str(k).strip().lower(): v for k, v in organ_classes.items()}\n",
    "\n",
    "# organ model\n",
    "organ_model = resnet18(pretrained=True)\n",
    "organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.load_state_dict(torch.load(\"organ_classifier_top997.pth\"))\n",
    "organ_model.eval().cuda()\n",
    "\n",
    "# CLIP model\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "clip_model = clip_model.cuda().eval()\n",
    "\n",
    "# 加载图像哈希\n",
    "known_hashes = np.load(npz_path)['hashes']\n",
    "\n",
    "# ==== Step 2: 构建 CLIP 检索索引 ====\n",
    "organ_index, organ_label_array = {}, {}\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    organ_df = df[df['organ'] == organ_name].reset_index(drop=True)\n",
    "    embeddings, labels = [], []\n",
    "    for _, row in tqdm(organ_df.iterrows(), total=len(organ_df), desc=f\"Indexing {organ_name}\"):\n",
    "        try:\n",
    "            image = preprocess(Image.open(row['image_path']).convert(\"RGB\")).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                emb = clip_model.encode_image(image)\n",
    "                emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(species_classes[row['label']])\n",
    "        except:\n",
    "            continue\n",
    "    if embeddings:\n",
    "        emb_arr = np.vstack(embeddings).astype(\"float32\")\n",
    "        index = faiss.IndexFlatIP(emb_arr.shape[1])\n",
    "        index.add(emb_arr)\n",
    "        organ_index[organ_id] = index\n",
    "        organ_label_array[organ_id] = np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5629094-c0ce-40f4-973a-481f7ad06bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------模型调试过程 --------\n",
    "## 用于检验上两块的数据加载部分的可行性\n",
    "## 数据加载部分！！@！通过检验！！！！！\n",
    "\n",
    "\n",
    "# ==== Step 3: 是否是植物判断（CLIP Zero-Shot） ====\n",
    "def is_plant_clip(image_path, threshold=0.3):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        #image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        \n",
    "        #text_tokens = open_clip.tokenize(text_labels).cuda()\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(device)\n",
    "        \n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    score = logits[0].item()  # 植物标签得分\n",
    "    print(\"🌱 是否植物得分:\", score)\n",
    "    return score >= threshold\n",
    "\n",
    "# ==== Step 4: 主推理函数 ====\n",
    "def predict_species(image_path):\n",
    "    if not is_plant_clip(image_path):\n",
    "        print(\"❌ 图像不是植物，请上传植物图像\")\n",
    "        return\n",
    "\n",
    "    with open(image_path, 'rb') as f:\n",
    "        img_hash = hashlib.md5(f.read()).hexdigest()\n",
    "    if img_hash not in known_hashes:\n",
    "        print(\"⚠️ 图像未在数据库中，无法识别\")\n",
    "        return\n",
    "\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    img_tensor = transform(image).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: 检索植物种类\n",
    "    clip_img = preprocess(image).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, 5)\n",
    "    result = [(id_to_species[labels[i]], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "    print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "    for name, score in result:\n",
    "        print(f\"✅ {name}（得分: {score:.4f}）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad4333d-361e-4e22-b5d1-011b7b6a28e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物得分: 0.5706782341003418\n",
      "🔍 识别为器官: flower\n",
      "✅ Populus tremula（得分: 1.0000）\n",
      "✅ Populus tremula（得分: 0.8554）\n",
      "✅ Acer negundo（得分: 0.8429）\n",
      "✅ Populus tremula（得分: 0.8401）\n",
      "✅ Acer negundo（得分: 0.8339）\n"
     ]
    }
   ],
   "source": [
    "# ==== Step 5: 测试 ====\n",
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\"\n",
    "predict_species(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ccb2fd-ae05-468c-bdca-6d8ca1db08fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物得分: 0.9678565263748169\n",
      "⚠️ 图像未在数据库中，无法识别\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\"\n",
    "predict_species(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6eaf1d-f523-40ea-9341-fec8b9666ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物得分: 0.9465965628623962\n",
      "⚠️ 图像未在数据库中，无法识别\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "predict_species(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1821c63-2251-44aa-bc5c-2be3fa4296e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物得分: 0.009094640612602234\n",
      "❌ 图像不是植物，请上传植物图像\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_keyboard.png\"\n",
    "predict_species(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ee1000-8736-4840-9562-3e0a67dc16e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物得分: 0.9744583368301392\n",
      "⚠️ 图像未在数据库中，无法识别\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\"\n",
    "predict_species(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67893f-f55a-4b9c-8d66-582740f1adf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 可用模型1的调试与优化部分：是否植物-是否在我数据中-植物判断\n",
    "#### 用 clip检索+fassi 向量、相似度\n",
    "##### 是否植物准确率约等于0.5-0.6（需调）； 是否在我数据中（一定误差）\n",
    "##### 暂时已调整好，整合至上文\n",
    "##### 但还需要调整 ！！！！植物种类的准确率需要提高，降低泛化误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "70989646-c0a8-4a45-bb71-51142c9c33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##============================= 功能1改进版----测试第一版 =========================####\n",
    "## ==============   正确版本----------是否植物-------是否在我的数据中？？？？？？？？？？-------识别植物 =================\n",
    "\n",
    "\n",
    "## ----------- 病虫害-----------病虫害线下大模型-----------------   for 节省token ================\n",
    "\n",
    "\n",
    "# ====== Step 4: 两阶段推理函数 ======\n",
    "\n",
    "# ====== 是否是植物======\n",
    "'''def is_plant_clip(image_path, threshold=0.4):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    score = logits[0].item()  # 植物标签得分\n",
    "    print(\"🌱 是否植物得分:\", score)\n",
    "    return score >= threshold'''\n",
    "\n",
    "'''def is_plant_clip(image_path):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    \n",
    "    top_idx = logits.argmax().item()\n",
    "    print(f\"🌱 是否植物判断：Top类 = {text_labels[top_idx]}, 得分 = {logits[top_idx]:.4f}\")\n",
    "    return top_idx == 0  # 0 对应 \"a photo of a plant\"'''\n",
    "\n",
    "#调整---------三---\n",
    "def is_plant_clip(image_path):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    top_idx = logits.argmax().item()\n",
    "    top_label = text_labels[top_idx]\n",
    "    print(f\"🌱 是否植物判断：Top类 = {top_label}, 得分 = {logits[top_idx]:.4f}\")\n",
    "    return top_label == \"a photo of a plant\"\n",
    "    score = logits[0].item()  # 植物标签得分\n",
    "    print(\"🌱 是否植物得分:\", score)\n",
    "    return score >= threshold\n",
    "\n",
    "\n",
    "# ====== 是否是我数据库的植物=======\n",
    "# ====== 植物预测=======\n",
    "'''def predict_species(image_path, sim_threshold=0.25, topk=5):\n",
    "    if not is_plant_clip(image_path):\n",
    "        print(\"❌ 图像不是植物，请上传植物图像\")\n",
    "        return\n",
    "\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 特征向量\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    # Step 3: 最近邻搜索\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "    top_score = D[0][0]\n",
    "\n",
    "    #print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "    if top_score < sim_threshold:\n",
    "        print(f\"⚠️ 相似度过低（最高得分 {top_score:.4f}），植物种类无法确认\")\n",
    "    else:\n",
    "        for j, i in enumerate(I[0]):\n",
    "            species = id_to_species[labels[i]]\n",
    "            score = float(D[0][j])\n",
    "            print(f\"✅ {species}（得分: {score:.4f}）\")'''\n",
    "\n",
    "'''def predict_species(image_path, sim_threshold=0.2, topk=5):\n",
    "    if not is_plant_clip(image_path):\n",
    "        print(\"❌ 图像不是植物，请上传植物图像\")\n",
    "        return\n",
    "\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 特征向量\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    # Step 3: 最近邻搜索\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "    top_score = D[0][0]\n",
    "    top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "    print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "\n",
    "    if top_score < sim_threshold:\n",
    "        print(f\"⚠️ 相似度过低（最高得分 {top_score:.4f}），植物种类无法确认\")\n",
    "    else:\n",
    "        print(f\"🌿 识别为植物种类: {top_label}（得分: {top_score:.4f}）\")\n",
    "        for j, i in enumerate(I[0]):\n",
    "            species = id_to_species[labels[i]]\n",
    "            score = float(D[0][j])\n",
    "            print(f\"✅ {species}（得分: {score:.4f}）\")'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------需要引入 拒识机制（Rejection）--------------\n",
    "#------------ 判断是否在我的数据集中--------\n",
    "\n",
    "'''    # Step 3: 最近邻搜索\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "\n",
    "    top1_score = float(D[0][0])\n",
    "    top2_score = float(D[0][1])\n",
    "    gap_score = top1_score - top2_score\n",
    "\n",
    "    print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "    print(f\"Top-1 相似度: {top1_score:.4f}, Top-2 相似度: {top2_score:.4f}, 差值: {gap_score:.4f}\")\n",
    "\n",
    "    # Rejection 判断逻辑\n",
    "    if top1_score < sim_threshold or gap_score < 0.0019:\n",
    "        print(f\"⚠️ 无法确认植物种类：可能不在数据库中\")\n",
    "        return\n",
    "    else:\n",
    "        top_label = id_to_species[labels[I[0][0]]]\n",
    "        print(f\"🌿 识别为植物种类: {top_label}（得分: {top1_score:.4f}）\")\n",
    "        for j, i in enumerate(I[0]):\n",
    "            species = id_to_species[labels[i]]\n",
    "            score = float(D[0][j])\n",
    "            print(f\"✅ {species}（得分: {score:.4f}）\")'''\n",
    "\n",
    "\n",
    "\n",
    "# 病虫害功能\n",
    "def is_diseased_clip_from_images_voting(image_paths, vote_threshold=0.7):\n",
    "\n",
    "    healthy_prompts = [\n",
    "        \"The leaves of healthy plants are usually bright green\",\n",
    "        \"The leaves of healthy plants are usually full and shiny, with no obvious signs of disease or insect damage on the leaf surface\",\n",
    "        \"Healthy plants usually have strong, straight stems that are able to support the weight of the plant\",\n",
    "        \"Healthy plants will show vigorous growth, including sprouting new leaves, extending branches and blooming flowers\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"A healthy plant has bright flowers with intact petals and no wilting, falling off, or diseased spots\"\n",
    "        \"The fruit of a healthy plant is full and has no cracks, rot or lesions. The fruit skin is normal color\"\n",
    "    ]\n",
    "    diseased_prompts = [\n",
    "        \"Unhealthy plant leaves or flowers will have spots or patches of different shapes, sizes and colors, such as round, oval, polygonal, wheel-shaped\",\n",
    "        \"Unhealthy plants have curled, shrunken, twisted leaves and flowers, and misshapen and stunted flowers\",\n",
    "        \"Tumor-like protrusions appear on the stem, such as rose cancer, and swelling occurs\",\n",
    "        'Soft rot, wet rot or dry rot on the stem',\n",
    "        \"Unhealthy plants may have holes, nicks, or signs of being eaten on their leaves and petals\",\n",
    "        \"Unhealthy plants may have visible insects, such as aphids and spider mites. Some pests will leave spider web-like silk\",\n",
    "        \"Leaves lose their normal green color, show yellowing symptoms, partially or completely die, and appear brown or black\",\n",
    "        \"The petals may appear water-soaked, rotten, softened, or even completely rotten.\"\n",
    "    ]\n",
    "\n",
    "    image_features_list = []\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                feat = clip_model.encode_image(image)\n",
    "                feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                image_features_list.append(feat)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 图像处理失败: {path}, 错误: {e}\")\n",
    "\n",
    "    if not image_features_list:\n",
    "        print(\"⚠️ 无有效图像特征，无法判断\")\n",
    "        return False\n",
    "\n",
    "    # 平均图像特征\n",
    "    image_features = torch.cat(image_features_list, dim=0).mean(dim=0, keepdim=True)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    diseased_votes = 0\n",
    "    total_votes = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h_prompt, d_prompt in zip(healthy_prompts, diseased_prompts):\n",
    "            text_tokens = open_clip.tokenize([h_prompt, d_prompt]).cuda()\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "            pred = logits.argmax().item()\n",
    "            total_votes += 1\n",
    "            if pred == 1:\n",
    "                diseased_votes += 1\n",
    "\n",
    "    ratio = diseased_votes / total_votes\n",
    "    print(f\"🦠 病虫害投票结果（融合图像）: {diseased_votes}/{total_votes}（比例: {ratio:.2f}）\")\n",
    "    return ratio >= vote_threshold\n",
    "\n",
    "\n",
    "#融合功能----主函数\n",
    "\n",
    "def batch_predict_species_with_disease(image_paths, topk=5, sim_threshold=0.5, vote_threshold=0.7):\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"❌ 输入图像列表为空\")\n",
    "        return\n",
    "\n",
    "    max_batch = 9\n",
    "    image_paths = image_paths[:max_batch]\n",
    "\n",
    "    is_plant_flags = [is_plant_clip(p) for p in image_paths]\n",
    "    num_plant = sum(is_plant_flags)\n",
    "    print(f\"🌿 共检测到 {num_plant}/{len(image_paths)} 张为植物图像\")\n",
    "\n",
    "    if num_plant <= len(image_paths) // 2:\n",
    "        print(\"❌ 大多数图像不是植物，无法识别\")\n",
    "        return\n",
    "\n",
    "    predicted_species = []\n",
    "    species_to_images = {}\n",
    "\n",
    "    for i, is_plant in enumerate(is_plant_flags):\n",
    "        if not is_plant:\n",
    "            continue\n",
    "        image_path = image_paths[i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor = organ_transform(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "            clip_img = preprocess(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                query_vec = clip_model.encode_image(clip_img)\n",
    "                query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "                query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "            index = organ_index[pred_organ_id]\n",
    "            labels = organ_label_array[pred_organ_id]\n",
    "            D, I = index.search(query_np, topk)\n",
    "\n",
    "            top_score = D[0][0]\n",
    "            top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "            if top_score >= sim_threshold:\n",
    "                predicted_species.append(top_label)\n",
    "                species_to_images.setdefault(top_label, []).append(image_path)\n",
    "            else:\n",
    "                predicted_species.append(\"unknown\")\n",
    "\n",
    "            # ✅ 单图模式下立即输出结果\n",
    "            if len(image_paths) == 1:\n",
    "                print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "                if top_score < sim_threshold:\n",
    "                    print(f\"⚠️ 相似度过低（最高得分 {top_score:.4f}），植物种类无法确认\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(f\"🌿 识别为植物种类: {top_label}（得分: {top_score:.4f}）\")\n",
    "                    has_disease = is_diseased_clip_from_images_voting([image_path], vote_threshold=vote_threshold)\n",
    "                    print(f\"🦠 病虫害判断: {'是' if has_disease else '否'}\")\n",
    "                    return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 图像处理失败: {image_path}, 错误: {e}\")\n",
    "            predicted_species.append(\"unknown\")\n",
    "\n",
    "    known_species = [s for s in predicted_species if s != \"unknown\"]\n",
    "    if len(image_paths) > 1 and len(known_species) <= len(predicted_species) // 2:\n",
    "        print(\"⚠️ 大多数植物图像无法识别为数据集中已知物种\")\n",
    "        return\n",
    "\n",
    "    counter = collections.Counter(known_species)\n",
    "    top3 = counter.most_common(3)\n",
    "    top1_species = top3[0][0]\n",
    "    disease_image_paths = species_to_images.get(top1_species, [])\n",
    "    has_disease = is_diseased_clip_from_images_voting(disease_image_paths, vote_threshold=vote_threshold)\n",
    "\n",
    "    print(\"🌼 投票结果：\")\n",
    "    for name, count in top3:\n",
    "        print(f\"✅ {name}: {count}票\")\n",
    "\n",
    "    print(f\"🦠 病虫害判断（Top1: {top1_species}）: {'是' if has_disease else '否'}（共 {len(disease_image_paths)} 张图像）\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f7b9c69c-d63f-46c3-9705-6f70bb01a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.5707\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: flower\n",
      "🌿 识别为植物种类: Populus tremula（得分: 1.0000）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害判断: 是\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b607de40-28bd-43ef-b9a5-aa3b488cc830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9679\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: leaf\n",
      "🌿 识别为植物种类: Populus tremula（得分: 0.8535）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害判断: 否\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "804b59bf-c162-4203-b831-538742034433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.8060\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: fruit\n",
      "🌿 识别为植物种类: Populus tremula（得分: 0.8189）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害判断: 否\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8f5c28fa-2434-4598-b176-7c0a26a6dfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9605\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: leaf\n",
      "🌿 识别为植物种类: Populus tremula（得分: 0.8713）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害判断: 否\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula2.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f2f76b4d-1904-4c90-94d3-c60fb990ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9725\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: leaf\n",
      "🌿 识别为植物种类: Acer rubrum（得分: 0.8476）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害判断: 否\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6fae1f15-65f2-486c-b387-b764fcd25934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of an object, 得分 = 0.8835\n",
      "🌿 共检测到 0/1 张为植物图像\n",
      "❌ 大多数图像不是植物，无法识别\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_keyboard.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1172a521-dcfd-42de-adf4-1ed3069fa169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9745\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: flower\n",
      "⚠️ 相似度过低（最高得分 0.8675），植物种类无法确认\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\"\n",
    "batch_predict_species_with_disease(test_image, sim_threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "139e785a-ebf5-4b0f-96a2-a5c1ee4ec863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9466\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🔍 识别为器官: leaf\n",
      "⚠️ 相似度过低（最高得分 0.7635），植物种类无法确认\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "batch_predict_species_with_disease(test_image, sim_threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9ee8e275-9aeb-45c9-8666-bab66c1996b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.5707\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9679\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.8060\n",
      "🌱 是否植物判断：Top类 = a photo of an object, 得分 = 0.8835\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9725\n",
      "🌿 共检测到 4/5 张为植物图像\n",
      "🦠 病虫害投票结果（融合图像）: 3/7（比例: 0.43）\n",
      "🌼 投票结果：\n",
      "✅ Populus tremula: 3票\n",
      "✅ Acer rubrum: 1票\n",
      "🦠 病虫害判断（Top1: Populus tremula）: 否（共 3 张图像）\n"
     ]
    }
   ],
   "source": [
    "# 示例：支持 1~9 张图片\n",
    "batch_predict_species_with_disease([\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_keyboard.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e853cc-2297-48c7-96dd-63cf4fe4c8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c32acb-be4b-43b0-aa02-f32d73e5caf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3da13d-7f26-4abc-ad47-1b1f4f8fb043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9670b72d-e05f-49db-8ad4-6a2aca885bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------增加多张图片功能 -----------\n",
    "\n",
    "import collections\n",
    "from typing import List\n",
    "\n",
    "def batch_predict_species(image_paths: List[str], topk=5, sim_threshold=0.2):\n",
    "    if not image_paths:\n",
    "        print(\"❌ 输入图像列表为空\")\n",
    "        return\n",
    "\n",
    "    max_batch = 9\n",
    "    image_paths = image_paths[:max_batch]\n",
    "    \n",
    "    # 1. 判断哪些图片是植物\n",
    "    is_plant_flags = [is_plant_clip(p) for p in image_paths]\n",
    "    num_plant = sum(is_plant_flags)\n",
    "    print(f\"🌿 共检测到 {num_plant}/{len(image_paths)} 张为植物图像\")\n",
    "\n",
    "    if num_plant <= len(image_paths) // 2:\n",
    "        print(\"❌ 大多数图像不是植物，无法识别\")\n",
    "        return\n",
    "\n",
    "    # 2. 仅对是植物的图像进行识别\n",
    "    predicted_species = []\n",
    "    for i, is_plant in enumerate(is_plant_flags):\n",
    "        if not is_plant:\n",
    "            continue\n",
    "        image_path = image_paths[i]\n",
    "        try:\n",
    "            # 器官分类\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "            # 特征提取 + 检索\n",
    "            clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                query_vec = clip_model.encode_image(clip_img)\n",
    "                query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "                query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "            index = organ_index[pred_organ_id]\n",
    "            labels = organ_label_array[pred_organ_id]\n",
    "            D, I = index.search(query_np, topk)\n",
    "\n",
    "            # 只考虑得分高的第一名\n",
    "            top_score = D[0][0]\n",
    "            top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "            if top_score >= sim_threshold:\n",
    "                predicted_species.append(top_label)\n",
    "            else:\n",
    "                predicted_species.append(\"unknown\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 图像处理失败: {image_path}, 错误: {e}\")\n",
    "            predicted_species.append(\"unknown\")\n",
    "\n",
    "    # 3. 判断是否大多数识别在数据集内\n",
    "    known_species = [s for s in predicted_species if s != \"unknown\"]\n",
    "    if len(known_species) <= len(predicted_species) // 3:\n",
    "        print(\"⚠️ 大多数植物图像无法识别为数据集中已知物种\")\n",
    "        return\n",
    "\n",
    "    # 4. 投票输出 Top3 物种\n",
    "    counter = collections.Counter(known_species)\n",
    "    top3 = counter.most_common(3)\n",
    "    print(\"🌼 投票结果：\")\n",
    "    for name, count in top3:\n",
    "        print(f\"✅ {name}: {count}票\")\n",
    "\n",
    "    return top3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8c7d297f-68c1-4976-8dfa-429204259439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.5707\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9679\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.8060\n",
      "🌱 是否植物判断：Top类 = a photo of an object, 得分 = 0.8835\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9725\n",
      "🌿 共检测到 4/5 张为植物图像\n",
      "⚠️ 大多数植物图像无法识别为数据集中已知物种\n"
     ]
    }
   ],
   "source": [
    "# 示例：支持 1~9 张图片\n",
    "batch_predict_species([\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_keyboard.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "], sim_threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "70f8a8a2-b95f-45e1-a1b7-c31b172abbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## =========== 病虫害可用 ==================\n",
    "\n",
    "def is_diseased_clip(image_path, diff_threshold=0.2):\n",
    "    labels = [\n",
    "        '''\"a healthy plant with green leaves and no spots\",\n",
    "        \"a plant with visible disease signs like spots, wilting or pests\"'''\n",
    "        \"The leaves of healthy plants are usually bright green\",\n",
    "        \"The leaves of healthy plants are usually full and shiny, with no obvious signs of disease or insect damage on the leaf surface\",\n",
    "        \"Healthy plants usually have strong, straight stems that are able to support the weight of the plant\",\n",
    "        \"Healthy plants will show vigorous growth, including sprouting new leaves, extending branches and blooming flowers\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"A healthy plant has bright flowers with intact petals and no wilting, falling off, or diseased spots\"\n",
    "        \"The fruit of a healthy plant is full and has no cracks, rot or lesions. The fruit skin is normal color\"\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        text_tokens = open_clip.tokenize(labels).to(device)\n",
    "\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "\n",
    "    healthy_score = logits[0].item()\n",
    "    disease_score = logits[1].item()\n",
    "    diff = disease_score - healthy_score\n",
    "\n",
    "    print(f\"🦠 健康得分: {healthy_score:.4f} | 病虫害得分: {disease_score:.4f} | 差值: {diff:.4f}\")\n",
    "\n",
    "    return diff > diff_threshold  # 超过阈值才认定为病虫害\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6bd8b3d5-c02d-46fa-abd0-e13517d45fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## =========== 病虫害可用预测主函数 ==================\n",
    "\n",
    "\n",
    "# 修复后批量识别函数（始终执行病虫害判断）\n",
    "def batch_predict_species_with_disease(image_paths: List[str], topk=5, sim_threshold=0.2):\n",
    "    if not image_paths:\n",
    "        print(\"❌ 输入图像列表为空\")\n",
    "        return\n",
    "\n",
    "    max_batch = 9\n",
    "    image_paths = image_paths[:max_batch]\n",
    "    \n",
    "    is_plant_flags = [is_plant_clip(p) for p in image_paths]\n",
    "    num_plant = sum(is_plant_flags)\n",
    "    print(f\"🌿 共检测到 {num_plant}/{len(image_paths)} 张为植物图像\")\n",
    "\n",
    "    if num_plant <= len(image_paths) // 2:\n",
    "        print(\"❌ 大多数图像不是植物，无法识别\")\n",
    "        return\n",
    "\n",
    "    predicted_species = []\n",
    "    disease_results = []\n",
    "    species_to_images = {}\n",
    "\n",
    "    for i, is_plant in enumerate(is_plant_flags):\n",
    "        if not is_plant:\n",
    "            continue\n",
    "        image_path = image_paths[i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "            clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                query_vec = clip_model.encode_image(clip_img)\n",
    "                query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "                query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "            index = organ_index[pred_organ_id]\n",
    "            labels = organ_label_array[pred_organ_id]\n",
    "            D, I = index.search(query_np, topk)\n",
    "\n",
    "            top_score = D[0][0]\n",
    "            top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "            print(f\"🎯 相似度得分: {top_score:.4f}\")\n",
    "\n",
    "            if top_score >= sim_threshold:\n",
    "                predicted_species.append(top_label)\n",
    "                species_to_images.setdefault(top_label, []).append(image_path)\n",
    "            else:\n",
    "                predicted_species.append(\"unknown\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 图像处理失败: {image_path}, 错误: {e}\")\n",
    "            predicted_species.append(\"unknown\")\n",
    "\n",
    "    known_species = [s for s in predicted_species if s != \"unknown\"]\n",
    "    if len(known_species) <= len(predicted_species) // 3:\n",
    "        print(\"⚠️ 大多数植物图像无法识别为数据集中已知物种\")\n",
    "        return\n",
    "\n",
    "    counter = collections.Counter(known_species)\n",
    "    top3 = counter.most_common(3)\n",
    "    # 🎯 选出出现次数最多的 top1 植物种类\n",
    "    #top1_species = most_common(predicted_species)\n",
    "    top1_species = top3[0][0]\n",
    "\n",
    "\n",
    "    # 🦠 将该种类所有图像提取出来，做病虫害判断\n",
    "    disease_image_paths = species_to_images.get(top1_species, [])\n",
    "\n",
    "    disease_yes = 0\n",
    "    for path in disease_image_paths:\n",
    "        if is_diseased_clip(path):\n",
    "            disease_yes += 1\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"🌼 投票结果：\")\n",
    "    for name, count in top3:\n",
    "        print(f\"✅ {name}: {count}票\")\n",
    "\n",
    "    '''if disease_results:\n",
    "        count_disease = disease_results.count(\"有病虫害\")\n",
    "        print(f\"🦠 病虫害识别结果：{count_disease}/{len(disease_results)} 张图像检测到病虫害\")\n",
    "\n",
    "    return top3, disease_results'''\n",
    "\n",
    "    #print(f\"✅ Top1: {top1_species}（得票数: {len(disease_image_paths)}）\")\n",
    "    print(f\"🦠 病虫害判断：{'是' if disease_yes > len(disease_image_paths) // 2 else '否'}（{disease_yes}/{len(disease_image_paths)} 张表现为病虫害）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8fe19719-b4f1-4098-be55-dadb9c719516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 共检测到 4/5 张为植物图像\n",
      "🎯 相似度得分: 1.0000\n",
      "🎯 相似度得分: 0.8535\n",
      "🎯 相似度得分: 0.8189\n",
      "🎯 相似度得分: 0.8476\n",
      "🦠 健康得分: 0.5380 | 病虫害得分: 0.0439 | 差值: -0.4941\n",
      "🦠 健康得分: 0.2524 | 病虫害得分: 0.2019 | 差值: -0.0505\n",
      "🌼 投票结果：\n",
      "✅ Populus tremula: 2票\n",
      "🦠 病虫害判断：否（0/2 张表现为病虫害）\n"
     ]
    }
   ],
   "source": [
    "# 示例：支持 1~9 张图片\n",
    "batch_predict_species_with_disease([\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_keyboard.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "], sim_threshold=0.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f5a011a1-7b36-43fc-8ec8-adbd3c36e307",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbatch_predict_species_with_disease\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[155], line 30\u001b[0m, in \u001b[0;36mbatch_predict_species_with_disease\u001b[0;34m(image_paths, topk, sim_threshold)\u001b[0m\n\u001b[1;32m     27\u001b[0m max_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     28\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m image_paths[:max_batch]\n\u001b[0;32m---> 30\u001b[0m is_plant_flags \u001b[38;5;241m=\u001b[39m [is_plant_clip(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m     31\u001b[0m num_plant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(is_plant_flags)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🌿 共检测到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_plant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 张为植物图像\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[155], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m max_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     28\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m image_paths[:max_batch]\n\u001b[0;32m---> 30\u001b[0m is_plant_flags \u001b[38;5;241m=\u001b[39m [\u001b[43mis_plant_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m     31\u001b[0m num_plant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(is_plant_flags)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🌿 共检测到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_plant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 张为植物图像\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m, in \u001b[0;36mis_plant_clip\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m text_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of a plant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of an animal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of a person\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of an object\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     image \u001b[38;5;241m=\u001b[39m preprocess(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m     text_tokens \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mtokenize(text_labels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mencode_image(image)\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/PIL/Image.py:3513\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m   3512\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m-> 3513\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3514\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/'"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "21149a50-e5dc-4698-8b8a-2e6de3b393a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_diseased_clip_from_images_voting(image_paths, vote_threshold=0.7):\n",
    "\n",
    "    healthy_prompts = [\n",
    "        \"The leaves of healthy plants are usually bright green\",\n",
    "        \"The leaves of healthy plants are usually full and shiny, with no obvious signs of disease or insect damage on the leaf surface\",\n",
    "        \"Healthy plants usually have strong, straight stems that are able to support the weight of the plant\",\n",
    "        \"Healthy plants will show vigorous growth, including sprouting new leaves, extending branches and blooming flowers\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"A healthy plant has bright flowers with intact petals and no wilting, falling off, or diseased spots\"\n",
    "        \"The fruit of a healthy plant is full and has no cracks, rot or lesions. The fruit skin is normal color\"\n",
    "    ]\n",
    "    diseased_prompts = [\n",
    "        \"Unhealthy plant leaves or flowers will have spots or patches of different shapes, sizes and colors, such as round, oval, polygonal, wheel-shaped\",\n",
    "        \"Unhealthy plants have curled, shrunken, twisted leaves and flowers, and misshapen and stunted flowers\",\n",
    "        \"Tumor-like protrusions appear on the stem, such as rose cancer, and swelling occurs\",\n",
    "        'Soft rot, wet rot or dry rot on the stem',\n",
    "        \"Unhealthy plants may have holes, nicks, or signs of being eaten on their leaves and petals\",\n",
    "        \"Unhealthy plants may have visible insects, such as aphids and spider mites. Some pests will leave spider web-like silk\",\n",
    "        \"Leaves lose their normal green color, show yellowing symptoms, partially or completely die, and appear brown or black\",\n",
    "        \"The petals may appear water-soaked, rotten, softened, or even completely rotten.\"\n",
    "    ]\n",
    "\n",
    "    image_features_list = []\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                feat = clip_model.encode_image(image)\n",
    "                feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                image_features_list.append(feat)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 图像处理失败: {path}, 错误: {e}\")\n",
    "\n",
    "    if not image_features_list:\n",
    "        print(\"⚠️ 无有效图像特征，无法判断\")\n",
    "        return False\n",
    "\n",
    "    # 平均图像特征\n",
    "    image_features = torch.cat(image_features_list, dim=0).mean(dim=0, keepdim=True)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    diseased_votes = 0\n",
    "    total_votes = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h_prompt, d_prompt in zip(healthy_prompts, diseased_prompts):\n",
    "            text_tokens = open_clip.tokenize([h_prompt, d_prompt]).cuda()\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "            pred = logits.argmax().item()\n",
    "            total_votes += 1\n",
    "            if pred == 1:\n",
    "                diseased_votes += 1\n",
    "\n",
    "    ratio = diseased_votes / total_votes\n",
    "    print(f\"🦠 病虫害投票结果（融合图像）: {diseased_votes}/{total_votes}（比例: {ratio:.2f}）\")\n",
    "    return ratio >= vote_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bfef99ca-76ac-4e08-bab2-1210b8db0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 假设这些变量和模型是已在环境中初始化的：\n",
    "# - is_plant_clip()\n",
    "# - organ_model\n",
    "# - organ_transform\n",
    "# - preprocess\n",
    "# - clip_model\n",
    "# - organ_index\n",
    "# - organ_label_array\n",
    "# - id_to_species\n",
    "# - species_classes\n",
    "# - is_diseased_clip_from_images_voting()\n",
    "\n",
    "def batch_predict_species_with_disease(image_paths, topk=5, sim_threshold=0.5, vote_threshold=0.7):\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"❌ 输入图像列表为空\")\n",
    "        return\n",
    "\n",
    "    max_batch = 9\n",
    "    image_paths = image_paths[:max_batch]\n",
    "\n",
    "    is_plant_flags = [is_plant_clip(p) for p in image_paths]\n",
    "    num_plant = sum(is_plant_flags)\n",
    "    print(f\"🌿 共检测到 {num_plant}/{len(image_paths)} 张为植物图像\")\n",
    "\n",
    "    if num_plant <= len(image_paths) // 2:\n",
    "        print(\"❌ 大多数图像不是植物，无法识别\")\n",
    "        return\n",
    "\n",
    "    predicted_species = []\n",
    "    species_to_images = {}\n",
    "\n",
    "    for i, is_plant in enumerate(is_plant_flags):\n",
    "        if not is_plant:\n",
    "            continue\n",
    "        image_path = image_paths[i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor = organ_transform(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "            clip_img = preprocess(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                query_vec = clip_model.encode_image(clip_img)\n",
    "                query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "                query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "            index = organ_index[pred_organ_id]\n",
    "            labels = organ_label_array[pred_organ_id]\n",
    "            D, I = index.search(query_np, topk)\n",
    "\n",
    "            top_score = D[0][0]\n",
    "            top_label = id_to_species[labels[I[0][0]]]\n",
    "\n",
    "            if top_score >= sim_threshold:\n",
    "                predicted_species.append(top_label)\n",
    "                species_to_images.setdefault(top_label, []).append(image_path)\n",
    "            else:\n",
    "                predicted_species.append(\"unknown\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 图像处理失败: {image_path}, 错误: {e}\")\n",
    "            predicted_species.append(\"unknown\")\n",
    "\n",
    "    known_species = [s for s in predicted_species if s != \"unknown\"]\n",
    "    if len(known_species) <= len(predicted_species) // 2:\n",
    "        print(\"⚠️ 大多数植物图像无法识别为数据集中已知物种\")\n",
    "        return\n",
    "\n",
    "    counter = collections.Counter(known_species)\n",
    "    top3 = counter.most_common(3)\n",
    "    top1_species = top3[0][0]\n",
    "\n",
    "    disease_image_paths = species_to_images.get(top1_species, [])\n",
    "    has_disease = is_diseased_clip_from_images_voting(disease_image_paths, vote_threshold=vote_threshold)\n",
    "\n",
    "    print(\"🌼 投票结果：\")\n",
    "    for name, count in top3:\n",
    "        print(f\"✅ {name}: {count}票\")\n",
    "\n",
    "    print(f\"🦠 病虫害判断（Top1: {top1_species}）: {'是' if has_disease else '否'}（共 {len(disease_image_paths)} 张图像）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e6acec81-9084-46aa-a2bb-3be55f0abd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.5707\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9679\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.8060\n",
      "🌱 是否植物判断：Top类 = a photo of an object, 得分 = 0.8835\n",
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9725\n",
      "🌿 共检测到 4/5 张为植物图像\n",
      "🦠 病虫害投票结果（融合图像）: 3/7（比例: 0.43）\n",
      "🌼 投票结果：\n",
      "✅ Populus tremula: 3票\n",
      "✅ Acer rubrum: 1票\n",
      "🦠 病虫害判断（Top1: Populus tremula）: 否（共 3 张图像）\n"
     ]
    }
   ],
   "source": [
    "# 示例：支持 1~9 张图片\n",
    "batch_predict_species_with_disease([\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_keyboard.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1227c6e6-343e-4281-b4b0-508d238faa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9591\n",
      "🌿 共检测到 1/1 张为植物图像\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🌼 投票结果：\n",
      "✅ Rumex crispus: 1票\n",
      "🦠 病虫害判断（Top1: Rumex crispus）: 否（共 1 张图像）\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_Rumex_crispus.jpg\"\n",
    "batch_predict_species_with_disease(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39572116-e8e9-4252-8a0c-a561e9ee0db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69ad28-72c3-4a3e-82da-6e593bafe0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "19afbdb9-f834-4dd3-8e77-8f1aa43a5be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦠 病虫害投票结果（融合图像）: 1/3（比例: 0.33）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例：支持 1~9 张图片\n",
    "is_diseased_clip_from_images_voting([\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_keyboard.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "419dfb8b-00ae-4a91-84bf-d89b908353d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.40 ➤ 假阳性率 = 40.00%（2/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.50 ➤ 假阳性率 = 40.00%（2/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.60 ➤ 假阳性率 = 20.00%（1/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.65 ➤ 假阳性率 = 20.00%（1/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.70 ➤ 假阳性率 = 20.00%（1/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.75 ➤ 假阳性率 = 20.00%（1/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.80 ➤ 假阳性率 = 20.00%（1/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.90 ➤ 假阳性率 = 0.00%（0/5）\n",
      "🦠 病虫害投票结果（融合图像）: 6/7（比例: 0.86）\n",
      "🦠 病虫害投票结果（融合图像）: 2/7（比例: 0.29）\n",
      "🦠 病虫害投票结果（融合图像）: 4/7（比例: 0.57）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🦠 病虫害投票结果（融合图像）: 1/7（比例: 0.14）\n",
      "🎯 阈值 = 0.95 ➤ 假阳性率 = 0.00%（0/5）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.4, 0.4),\n",
       " (0.5, 0.4),\n",
       " (0.6, 0.2),\n",
       " (0.65, 0.2),\n",
       " (0.7, 0.2),\n",
       " (0.75, 0.2),\n",
       " (0.8, 0.2),\n",
       " (0.9, 0.0),\n",
       " (0.95, 0.0)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ====== 评估病虫害假阳性 =====\n",
    "\n",
    "healthy_image_paths = [\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_inside_Populus tremula.jpg\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\",\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula1.png\",  \n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula2.png\",# 非植物\n",
    "    \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula3.png\"\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_false_positive_rate(image_paths, thresholds=[0.5, 0.6, 0.7, 0.8,0.9,0.95]):\n",
    "    results = []\n",
    "    for th in thresholds:\n",
    "        false_positive = 0\n",
    "        total = 0\n",
    "\n",
    "        for path in image_paths:\n",
    "            try:\n",
    "                pred = is_diseased_clip_from_images_voting([path], vote_threshold=th)\n",
    "                if pred:  # 预测为“有病虫害”\n",
    "                    false_positive += 1\n",
    "                total += 1\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 图像处理失败: {path} 错误: {e}\")\n",
    "\n",
    "        fpr = false_positive / total if total > 0 else 0\n",
    "        print(f\"🎯 阈值 = {th:.2f} ➤ 假阳性率 = {fpr:.2%}（{false_positive}/{total}）\")\n",
    "        results.append((th, fpr))\n",
    "\n",
    "    return results\n",
    "    \n",
    "thresholds = [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 0.8,0.9,0.95]\n",
    "evaluate_false_positive_rate(healthy_image_paths, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ee2b1-7a57-4c96-9459-68aca9e15fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90efbfe-b5c7-46f2-a656-f868d09298c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd4ff6-342c-4756-8716-5df6fcc739ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d0db1-524c-412d-9abe-537b5ea33c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3babc-68e5-45f0-ba3b-20e2cddb2292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746004f-07e6-44c6-9bc8-75e7bc7226e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66b5c424-553c-4501-986a-2797d39b5fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9745\n",
      "🔍 识别为器官: flower\n",
      "Top-1 相似度: 0.8675, Top-2 相似度: 0.8551, 差值: 0.0123\n",
      "⚠️ 无法确认植物种类：可能不在数据库中\n"
     ]
    }
   ],
   "source": [
    "# 以下为自己图片的重测\n",
    "test_image = \"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\"\n",
    "predict_species(test_image, sim_threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b8bc081-3e43-4285-9012-a06bac71f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9466\n",
      "🔍 识别为器官: leaf\n",
      "Top-1 相似度: 0.7635, Top-2 相似度: 0.7502, 差值: 0.0133\n",
      "⚠️ 无法确认植物种类：可能不在数据库中\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_leaf.png\"\n",
    "predict_species(test_image, sim_threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8334c98-777f-4886-ad72-dcc4798d5174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9679\n",
      "🔍 识别为器官: leaf\n",
      "Top-1 相似度: 0.8535, Top-2 相似度: 0.8516, 差值: 0.0019\n",
      "⚠️ 无法确认植物种类：可能不在数据库中\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\"\n",
    "predict_species(test_image, sim_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89f90162-3335-4d01-8721-57baaf24f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.5383\n",
      "🔍 识别为器官: bark\n",
      "Top-1 相似度: 0.3032, Top-2 相似度: 0.2862, 差值: 0.0170\n",
      "⚠️ 无法确认植物种类：可能不在数据库中\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_bottel.png\"\n",
    "predict_species(test_image, sim_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "000b429e-d595-4622-9199-e2861844d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of an object, 得分 = 0.8835\n",
      "❌ 图像不是植物，请上传植物图像\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_keyboard.png\"\n",
    "predict_species(test_image, sim_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f23116e2-f428-451f-a5ed-da5e14e1364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.9679\n",
      "🔍 识别为器官: leaf\n",
      "Top-1 相似度: 0.8535, Top-2 相似度: 0.8516, 差值: 0.0019\n",
      "⚠️ 无法确认植物种类：可能不在数据库中\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\"\n",
    "predict_species(test_image, sim_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65cb7b3c-1765-40fb-b8ee-e718536eac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物判断：Top类 = a photo of a plant, 得分 = 0.5707\n",
      "🔍 识别为器官: flower\n",
      "Top-1 相似度: 1.0000, Top-2 相似度: 0.8554, 差值: 0.1446\n",
      "🌿 识别为植物种类: Populus tremula（得分: 1.0000）\n",
      "✅ Populus tremula（得分: 1.0000）\n",
      "✅ Populus tremula（得分: 0.8554）\n",
      "✅ Acer negundo（得分: 0.8429）\n",
      "✅ Populus tremula（得分: 0.8401）\n",
      "✅ Acer negundo（得分: 0.8339）\n"
     ]
    }
   ],
   "source": [
    "test_image = \"/mnt/e/code/plants-classification-conda/test_997_Populus_tremula.png\"\n",
    "predict_species(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77926359-9d0c-48b5-88d5-3bc165edc4e3",
   "metadata": {},
   "source": [
    "### 新模型： resnet50-代替clip+fassi，训练resnet50 判断植物种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "774bf7fd-97a2-4616-9e39-281138571911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Species classification: 19955 train, 4989 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_31370/3404665672.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('organ_classifier_top997.pth', map_location=device)\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ================================ 新模型测试 - Step3 ===============================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ====== Step 1: 初始化设备与数据 ======\n",
    "csv_path     = \"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\"\n",
    "#organ_json   = \"/mnt/e/code/plants-classification-conda/organ_classes_top997.json\"\n",
    "#species_json = \"/mnt/e/code/plants-classification-conda/species_classes_top997.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df     = pd.read_csv(csv_path)\n",
    "\n",
    "'''with open(organ_json, 'r') as f:\n",
    "    organ_classes = {str(k).strip().lower(): v for k, v in json.load(f).items()}\n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {int(v): k for k, v in species_classes.items()}'''\n",
    "#id_to_organ   = {int(v): k for k, v in organ_classes.items()}\n",
    "\n",
    "# ===== MODIFIED 1: 划分训练集和测试集 =====\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "print(f\"-> Species classification: {len(train_df)} train, {len(test_df)} test\")\n",
    "\n",
    "# ====== Step 2: 定义 Transform ======\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "species_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ====== Step 3: OrganClassifier 加载（不变） ======\n",
    "'''class OrganDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = organ_classes[str(row['organ']).strip().lower()]\n",
    "        return img, label'''\n",
    "\n",
    "# Organ model\n",
    "organ_model = models.resnet18(pretrained=True)\n",
    "#organ_model.fc = nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.fc = nn.Linear(organ_model.fc.in_features, len(train_df['organ_id'].unique()))\n",
    "organ_model.load_state_dict(\n",
    "    torch.load('organ_classifier_top997.pth', map_location=device)\n",
    ")\n",
    "organ_model.to(device).eval()\n",
    "\n",
    "# ====== Step 4: 加载 CLIP，用于植物/非植物 & 病虫害 ======\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    'ViT-L-14', pretrained='openai'\n",
    ")\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6511bc73-7dc0-4c8f-b293-25d22f7c3f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c0ca398-daa1-4df9-a587-cc73f3d0e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ_id</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17291</th>\n",
       "      <td>Prunus virginiana</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Pr...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8840</th>\n",
       "      <td>Caesalpinia pulcherrima</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>Vitex agnus-castus</td>\n",
       "      <td>bark</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772</th>\n",
       "      <td>Reynoutria japonica</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Re...</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7073</th>\n",
       "      <td>Acer negundo</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ac...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8976</th>\n",
       "      <td>Caesalpinia pulcherrima</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ca...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12697</th>\n",
       "      <td>Sorbus aucuparia</td>\n",
       "      <td>bark</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/So...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11213</th>\n",
       "      <td>Scandosorbus intermedia</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Sc...</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18917</th>\n",
       "      <td>Campsis radicans</td>\n",
       "      <td>fruit</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>Reynoutria japonica</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Re...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19955 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label   organ  \\\n",
       "17291        Prunus virginiana    leaf   \n",
       "8840   Caesalpinia pulcherrima  flower   \n",
       "19976       Vitex agnus-castus    bark   \n",
       "5772       Reynoutria japonica    leaf   \n",
       "7073              Acer negundo  flower   \n",
       "...                        ...     ...   \n",
       "8976   Caesalpinia pulcherrima    leaf   \n",
       "12697         Sorbus aucuparia    bark   \n",
       "11213  Scandosorbus intermedia    leaf   \n",
       "18917         Campsis radicans   fruit   \n",
       "5575       Reynoutria japonica  flower   \n",
       "\n",
       "                                              image_path  organ_id  species  \n",
       "17291  /mnt/zshare/plants/Plant_Data/top5000_china/Pr...         3       11  \n",
       "8840   /mnt/zshare/plants/Plant_Data/top5000_china/Ca...         1        5  \n",
       "19976  /mnt/zshare/plants/Plant_Data/top5000_china/Vi...         0       24  \n",
       "5772   /mnt/zshare/plants/Plant_Data/top5000_china/Re...         3       13  \n",
       "7073   /mnt/zshare/plants/Plant_Data/top5000_china/Ac...         1        1  \n",
       "...                                                  ...       ...      ...  \n",
       "8976   /mnt/zshare/plants/Plant_Data/top5000_china/Ca...         3        5  \n",
       "12697  /mnt/zshare/plants/Plant_Data/top5000_china/So...         0       21  \n",
       "11213  /mnt/zshare/plants/Plant_Data/top5000_china/Sc...         3       18  \n",
       "18917  /mnt/zshare/plants/Plant_Data/top5000_china/Ca...         2        6  \n",
       "5575   /mnt/zshare/plants/Plant_Data/top5000_china/Re...         1       13  \n",
       "\n",
       "[19955 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5b556dc4-4cbd-43c7-a952-f95aed16cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 植物检测函数（保持不变）\n",
    "def train_is_plant_clip(image_path):\n",
    "    # 确认路径存在且不是目录\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"⚠️ 路径不存在: {image_path}\")\n",
    "        return False\n",
    "    if os.path.isdir(image_path):\n",
    "        print(f\"⚠️ 路径是目录, 不是图片文件: {image_path}\")\n",
    "        return False\n",
    "\n",
    "    labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    image = preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    text_tokens = open_clip.tokenize(labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feat = clip_model.encode_image(image)\n",
    "        txt_feat = clip_model.encode_text(text_tokens)\n",
    "    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "    txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n",
    "    logits = (100 * img_feat @ txt_feat.T).softmax(dim=-1).squeeze()\n",
    "    return logits.argmax().item() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28a78443-79b7-4b62-9002-7ce7e4860552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████████████████████████████████████████████████| 312/312 [03:14<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc=0.6652, Val Acc=0.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████████████████████████████████████████████████| 312/312 [03:21<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc=0.8786, Val Acc=0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████████████████████████████████████████████████| 312/312 [03:09<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc=0.9469, Val Acc=0.8136\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "        Acacia dealbata       0.75      0.84      0.79       199\n",
      "           Acer negundo       0.75      0.77      0.76       200\n",
      "            Acer rubrum       0.75      0.71      0.73       200\n",
      "      Berberis japonica       0.92      0.83      0.87       199\n",
      "       Buddleja davidii       0.75      0.79      0.77       200\n",
      "Caesalpinia pulcherrima       0.82      0.87      0.84       200\n",
      "       Campsis radicans       0.86      0.85      0.86       199\n",
      "      Elaeagnus pungens       0.82      0.91      0.86       200\n",
      "      Equisetum arvense       0.83      0.96      0.90       199\n",
      "       Malva sylvestris       0.95      0.85      0.90       199\n",
      "        Populus tremula       0.81      0.84      0.83       199\n",
      "      Prunus virginiana       0.75      0.66      0.70       200\n",
      "    Pyracantha coccinea       0.74      0.91      0.82       199\n",
      "    Reynoutria japonica       0.76      0.82      0.79       200\n",
      "           Rhus typhina       0.88      0.78      0.82       200\n",
      "       Ricinus communis       0.94      0.82      0.87       200\n",
      "          Rumex crispus       0.86      0.80      0.83       200\n",
      "      Sambucus racemosa       0.70      0.74      0.72       200\n",
      "Scandosorbus intermedia       0.90      0.65      0.75       199\n",
      "       Senecio vulgaris       0.85      0.93      0.89       200\n",
      "       Sonchus arvensis       0.81      0.85      0.83       199\n",
      "       Sorbus aucuparia       0.76      0.79      0.78       200\n",
      "        Viburnum opulus       0.89      0.70      0.78       199\n",
      "         Viburnum tinus       0.84      0.79      0.81       200\n",
      "     Vitex agnus-castus       0.79      0.86      0.82       199\n",
      "\n",
      "               accuracy                           0.81      4989\n",
      "              macro avg       0.82      0.81      0.81      4989\n",
      "           weighted avg       0.82      0.81      0.81      4989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== MODIFIED 2: SpeciesClassifier =====\n",
    "#暂时保留\n",
    "\n",
    "class SpeciesDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, int(row['species'])\n",
    "\n",
    "species_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_ds = SpeciesDataset(train_df, species_transform)\n",
    "test_ds  = SpeciesDataset(test_df, species_transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
    "\n",
    "species_model = models.resnet50(pretrained=True)\n",
    "#species_model.fc = nn.Linear(species_model.fc.in_features, len(species_classes))\n",
    "species_model.fc = nn.Linear(species_model.fc.in_features, len(df['species'].unique())) \n",
    "species_model.to(device)\n",
    "optimizer = torch.optim.Adam(species_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    species_model.train()\n",
    "    total, correct, running_loss = 0,0,0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outs = species_model(imgs)\n",
    "        loss = criterion(outs, labels)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        preds = outs.argmax(1)\n",
    "        correct += (preds==labels).sum().item(); total += labels.size(0)\n",
    "        running_loss += loss.item()*labels.size(0)\n",
    "    # 验证\n",
    "    val_correct, val_loss, val_total = 0,0,0\n",
    "    species_model.eval()\n",
    "    all_pred, all_lbl = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outs = species_model(imgs)\n",
    "            loss = criterion(outs, labels)\n",
    "            preds = outs.argmax(1)\n",
    "            val_correct += (preds==labels).sum().item(); val_total += labels.size(0)\n",
    "            val_loss += loss.item()*labels.size(0)\n",
    "            all_pred.extend(preds.cpu().numpy()); all_lbl.extend(labels.cpu().numpy())\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={correct/total:.4f}, Val Acc={val_correct/val_total:.4f}\")\n",
    "\n",
    "print(classification_report(all_lbl, all_pred, target_names=[id_to_species[i] for i in range(len(id_to_species))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "569ef1f2-7489-4f08-89dc-e3c995599dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已保存 species_model state dict 到 ./species_model_top997.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存 State Dict\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "torch.save(species_model.state_dict(), './species_model_top997.pth')\n",
    "print(\"✅ 已保存 species_model state dict 到 ./species_model_top997.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9fd0686-88ed-4ded-ba47-b3fc245b205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31370/1657823549.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('species_model_top997.pth', map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== Step 5: SpeciesClassifier 训练 & 加载 ======\n",
    "class SpeciesDataset1111(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = int(row['species'])\n",
    "        return img, label\n",
    "\n",
    "# 培训代码略，请确保已经保存了模型 State Dict\n",
    "species_model = models.resnet50(pretrained=True)\n",
    "species_model.fc = nn.Linear(species_model.fc.in_features, len(species_classes))\n",
    "species_model.load_state_dict(\n",
    "    torch.load('species_model_top997.pth', map_location=device)\n",
    ")\n",
    "species_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "668f6d8d-ac11-415a-bc86-b56162df55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ORIGINAL: 病虫害检测函数（保持不变） =====\n",
    "def is_diseased_train(image_paths, vote_threshold=0.7):\n",
    "    # ... original voting prompts and code unchanged ...\n",
    "    healthy_prompts = [\n",
    "        \"The leaves of healthy plants are usually bright green\",\n",
    "        \"The leaves of healthy plants are usually full and shiny, with no obvious signs of disease or insect damage on the leaf surface\",\n",
    "        \"Healthy plants usually have strong, straight stems that are able to support the weight of the plant\",\n",
    "        \"Healthy plants will show vigorous growth, including sprouting new leaves, extending branches and blooming flowers\",\n",
    "        \"Healthy plant leaves have clear veins and are not excessively curled or wrinkled\",\n",
    "        \"A healthy plant has bright flowers with intact petals and no wilting, falling off, or diseased spots\",\n",
    "        \"The fruit of a healthy plant is full and has no cracks, rot or lesions. The fruit skin is normal color\"\n",
    "    ]\n",
    "    diseased_prompts = [\n",
    "        \"Unhealthy plant leaves or flowers will have spots or patches of different shapes, sizes and colors, such as round, oval, polygonal, wheel-shaped\",\n",
    "        \"Unhealthy plants have curled, shrunken, twisted leaves and flowers, and misshapen and stunted flowers\",\n",
    "        \"Tumor-like protrusions appear on the stem, such as rose cancer, and swelling occurs\",\n",
    "        'Soft rot, wet rot or dry rot on the stem',\n",
    "        \"Unhealthy plants may have holes, nicks, or signs of being eaten on their leaves and petals\",\n",
    "        \"Unhealthy plants may have visible insects, such as aphids and spider mites. Some pests will leave spider web-like silk\",\n",
    "        \"Leaves lose their normal green color, show yellowing symptoms, partially or completely die, and appear brown or black\",\n",
    "        \"The petals may appear water-soaked, rotten, softened, or even completely rotten.\"\n",
    "    ]\n",
    "    feats = []\n",
    "    for p in image_paths:\n",
    "        try:\n",
    "            img = preprocess(Image.open(p).convert('RGB')).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                f = clip_model.encode_image(img)\n",
    "                feats.append(f / f.norm(dim=-1, keepdim=True))\n",
    "        except:\n",
    "            continue\n",
    "    if not feats:\n",
    "        print(\"⚠️ 无有效图像特征，无法判断病虫害\")\n",
    "        return False\n",
    "    img_feat = torch.mean(torch.stack(feats), dim=0, keepdim=True)\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "    votes, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for hp, dp in zip(healthy_prompts, diseased_prompts):\n",
    "            toks = open_clip.tokenize([hp, dp]).to(device)\n",
    "            txt_feat = clip_model.encode_text(toks)\n",
    "            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "            logit = (100 * img_feat @ txt_feat.T).softmax(dim=-1).squeeze()\n",
    "            total += 1\n",
    "            votes += (logit.argmax().item() == 1)\n",
    "    return (votes / total) >= vote_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4bf45ec3-9d45-4dd9-bc48-7066c37ec031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 7: 推理函数修正 ======\n",
    "def train_predict_species(image_paths, prob_threshold=0.5, vote_threshold=0.7):\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    for path in image_paths:\n",
    "        # 路径检查\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"❌ 文件不存在: {path}\")\n",
    "            continue\n",
    "        if os.path.isdir(path):\n",
    "            print(f\"❌ 路径是目录, 不是图片文件: {path}\")\n",
    "            continue\n",
    "        # 1. 是否植物\n",
    "        try:\n",
    "            if not train_is_plant_clip(path):\n",
    "                print(f\"❌ {path} -- 不是植物\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 无法判断是否植物: {e}\")\n",
    "            continue\n",
    "        # 2. 器官判定\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        organ_id = organ_model(\n",
    "            organ_transform(img).unsqueeze(0).to(device)\n",
    "        ).argmax(1).item()\n",
    "        # 3. 种类判定\n",
    "        with torch.no_grad():\n",
    "            out = species_model(\n",
    "                species_transform(img).unsqueeze(0).to(device)\n",
    "            )\n",
    "            probs = F.softmax(out, dim=-1).cpu().squeeze()\n",
    "        top_prob, top_idx = probs.topk(1)\n",
    "        if top_prob.item() < prob_threshold:\n",
    "            print(f\"⚠️ {path} -- 无法识别（置信度 {top_prob.item():.4f}）\")\n",
    "            continue\n",
    "        species_pred = f\"Species {top_idx.item()}\"\n",
    "        print(f\"🌿 {path} -- 识别为: {species_pred} (置信度: {top_prob.item():.4f})\")\n",
    "        # 4. 病虫害\n",
    "        try:\n",
    "            res = is_diseased_train([path], vote_threshold)\n",
    "            print(f\"🦠 病虫害: {'是' if res else '否'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 病虫害判断失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888901f-e94e-4025-be53-104e35c4cc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f48594-7c33-495c-8a61-d190c659340a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ab6f037c-33a4-49ce-b94d-9f6041beb8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ /mnt/e/code/plants-classification-conda/test_leaf.png -- 无法识别（置信度 0.3803）\n"
     ]
    }
   ],
   "source": [
    "train_predict_species(\"/mnt/e/code/plants-classification-conda/test_leaf.png\", prob_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "18260ef5-7f33-4790-88c9-10fab4d2785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 /mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png -- 识别为: Species 24 (置信度: 0.6837)\n",
      "🦠 病虫害: 否\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\"\n",
    "train_predict_species(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b1aa8a4-bce7-448d-8727-9804e517075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 /mnt/e/code/plants-classification-conda/test_Rumex_crispus.jpg -- 识别为: Species 16 (置信度: 0.9814)\n",
      "🦠 病虫害: 否\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/mnt/e/code/plants-classification-conda/test_Rumex_crispus.jpg\"\n",
    "train_predict_species(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a112768b-3ab5-4ae7-8d37-7be5f43649b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ /mnt/e/code/plants-classification-conda/test_keyboard.png -- 不是植物\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/mnt/e/code/plants-classification-conda/test_keyboard.png\"\n",
    "train_predict_species(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce6e4cec-337c-43bb-80ef-adaaf28fc12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌿 /mnt/e/code/plants-classification-conda/test_Rumex_crispus.jpg -- 识别为: Species 16 (置信度: 0.9814)\n",
      "🦠 病虫害: 否\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/mnt/e/code/plants-classification-conda/test_Rumex_crispus.jpg\"\n",
    "train_predict_species(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5de40-20f4-4741-9475-c8efdbd0bc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62184bf0-67f4-48ef-b66d-0efec59e7e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fc4a0e-5a8f-4dcb-8e67-896c724b1306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始训练是否是植物分类器...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PlantBinary Epoch 1: 100%|████████████████████████| 780/780 [05:20<00:00,  2.43it/s, acc=0.999, loss=0.0021]\n",
      "PlantBinary Epoch 2: 100%|███████████████████████████| 780/780 [05:23<00:00,  2.41it/s, acc=1, loss=1.74e-5]\n",
      "PlantBinary Epoch 3: 100%|███████████████████████████| 780/780 [05:22<00:00,  2.42it/s, acc=1, loss=6.22e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 是否是植物模型训练完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始训练植物种类分类模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Species Epoch 1:   0%|                                                              | 0/390 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'species_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'species_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 157\u001b[0m\n\u001b[1;32m    155\u001b[0m total, correct, running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    156\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(species_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecies Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[1;32m    158\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m species_model(imgs)\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 134\u001b[0m, in \u001b[0;36mSpeciesDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    132\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m--> 134\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspecies_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/pandas/core/series.py:1130\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/pandas/core/series.py:1246\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/plant310/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'species_id'"
     ]
    }
   ],
   "source": [
    "# ✅ 修复后的两阶段植物识别系统：OrganClassifier + SpeciesClassifier\n",
    "# 结构顺序修正，确保 df 加载和是否植物模型构建无错\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# ====== Step 1: 初始化设备与数据 ======\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_num_top997.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes_top997.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes_top997.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "\n",
    "# ✅ 使用植物图像与 torchvision 自带的 CIFAR10 非植物图像构建二分类数据集\n",
    "plant_images = df['image_path'].tolist()\n",
    "\n",
    "cifar_dataset = CIFAR10(root='./data', train=True, download=True)\n",
    "cifar_images = []\n",
    "for i in range(len(cifar_dataset)):\n",
    "    img, label = cifar_dataset[i]\n",
    "    if label in [1, 2, 3, 4, 5, 7, 8, 9]:  # 非植物类别\n",
    "        path = f\"./data/cifar_img_{i}.jpg\"\n",
    "        img.save(path)\n",
    "        cifar_images.append(path)\n",
    "    if len(cifar_images) >= len(plant_images):\n",
    "        break\n",
    "\n",
    "nonplant_images = cifar_images\n",
    "binary_image_paths = plant_images + nonplant_images\n",
    "binary_labels = [1]*len(plant_images) + [0]*len(nonplant_images)\n",
    "\n",
    "binary_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class PlantBinaryDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "binary_dataset = PlantBinaryDataset(binary_image_paths, binary_labels, binary_transform)\n",
    "binary_loader = DataLoader(binary_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "isplant_model = models.resnet18(pretrained=True)\n",
    "isplant_model.fc = nn.Linear(isplant_model.fc.in_features, 2)\n",
    "isplant_model = isplant_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(isplant_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"🚀 开始训练是否是植物分类器...\")\n",
    "for epoch in range(3):\n",
    "    isplant_model.train()\n",
    "    total, correct, loss_total = 0, 0, 0\n",
    "    loop = tqdm(binary_loader, desc=f\"PlantBinary Epoch {epoch+1}\")\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = isplant_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_total += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=loss_total/total)\n",
    "print(\"✅ 是否是植物模型训练完成\")\n",
    "\n",
    "torch.save(isplant_model.state_dict(), \"isplant_model.pth\")\n",
    "\n",
    "# 测试函数\n",
    "'''def is_plant_image(img_path, threshold=0.6):\n",
    "    binary_test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = binary_test_transform(image).unsqueeze(0).to(device)\n",
    "    isplant_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = isplant_model(image_tensor)\n",
    "        prob = F.softmax(output, dim=1)[0][1].item()\n",
    "    print(f\"🌿 是否是植物得分: {prob:.4f}\")\n",
    "    return prob >= threshold'''\n",
    "\n",
    "# ====== Step 3: 构建植物种类分类模型（替代 CLIP 检索） ======\n",
    "class SpeciesDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = int(row['species_id'])\n",
    "        return image, label\n",
    "\n",
    "species_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "species_dataset = SpeciesDataset(df, species_transform)\n",
    "species_loader = DataLoader(species_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "species_model = models.resnet50(pretrained=True)\n",
    "species_model.fc = nn.Linear(species_model.fc.in_features, len(species_classes))\n",
    "species_model = species_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(species_model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"🚀 开始训练植物种类分类模型...\")\n",
    "for epoch in range(3):\n",
    "    species_model.train()\n",
    "    total, correct, running_loss = 0, 0, 0\n",
    "    loop = tqdm(species_loader, desc=f\"Species Epoch {epoch+1}\")\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = species_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=running_loss/total)\n",
    "print(\"✅ 植物种类模型训练完成\")\n",
    "\n",
    "# ====== Step 4: 两阶段推理函数 ======\n",
    "def predict_species(image_path, confidence_threshold=0.6):\n",
    "    if not is_plant_image(image_path, threshold=confidence_threshold):\n",
    "        print(\"❌ 图像不是植物，请上传植物图像\")\n",
    "        return\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Step 1: 预测器官\n",
    "    img_tensor = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: 预测植物种类\n",
    "    species_tensor = species_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = species_model(species_tensor)\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "        topk = torch.topk(probs, k=3)\n",
    "        top_probs = topk.values.cpu().numpy()\n",
    "        top_indices = topk.indices.cpu().numpy()\n",
    "\n",
    "    print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "    for i in range(len(top_probs)):\n",
    "        name = id_to_species[top_indices[i]]\n",
    "        print(f\"🌿 候选: {name}（置信度: {top_probs[i]:.4f}）\")\n",
    "\n",
    "    if top_probs[0] < confidence_threshold:\n",
    "        print(f\"⚠️ 无法确认植物种类（最高置信度 {top_probs[0]:.4f}）\")\n",
    "        return\n",
    "\n",
    "    best_species = id_to_species[top_indices[0]]\n",
    "    print(f\"✅ 识别为植物种类: {best_species}（置信度 {top_probs[0]:.4f}）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae39e4-f4e7-48a7-9523-336abb9ce866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbe116-ebaf-4503-9ee3-999a5870c2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8fb866fe-1b33-4ca3-85ce-973cc790000c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ_id</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sonchus arvensis</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/So...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sonchus arvensis</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/So...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sonchus arvensis</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/So...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sonchus arvensis</td>\n",
       "      <td>leaf</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/So...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sonchus arvensis</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/So...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24939</th>\n",
       "      <td>Viburnum tinus</td>\n",
       "      <td>fruit</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Vi...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24940</th>\n",
       "      <td>Viburnum tinus</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24941</th>\n",
       "      <td>Viburnum tinus</td>\n",
       "      <td>flower</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24942</th>\n",
       "      <td>Viburnum tinus</td>\n",
       "      <td>fruit</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Vi...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24943</th>\n",
       "      <td>Viburnum tinus</td>\n",
       "      <td>bark</td>\n",
       "      <td>/mnt/zshare/plants/Plant_Data/top5000_china/Vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24944 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  label   organ  \\\n",
       "0      Sonchus arvensis    leaf   \n",
       "1      Sonchus arvensis  flower   \n",
       "2      Sonchus arvensis    leaf   \n",
       "3      Sonchus arvensis    leaf   \n",
       "4      Sonchus arvensis  flower   \n",
       "...                 ...     ...   \n",
       "24939    Viburnum tinus   fruit   \n",
       "24940    Viburnum tinus  flower   \n",
       "24941    Viburnum tinus  flower   \n",
       "24942    Viburnum tinus   fruit   \n",
       "24943    Viburnum tinus    bark   \n",
       "\n",
       "                                              image_path  organ_id  species  \n",
       "0      /mnt/zshare/plants/Plant_Data/top5000_china/So...         3       20  \n",
       "1      /mnt/zshare/plants/Plant_Data/top5000_china/So...         1       20  \n",
       "2      /mnt/zshare/plants/Plant_Data/top5000_china/So...         3       20  \n",
       "3      /mnt/zshare/plants/Plant_Data/top5000_china/So...         3       20  \n",
       "4      /mnt/zshare/plants/Plant_Data/top5000_china/So...         1       20  \n",
       "...                                                  ...       ...      ...  \n",
       "24939  /mnt/zshare/plants/Plant_Data/top5000_china/Vi...         2       23  \n",
       "24940  /mnt/zshare/plants/Plant_Data/top5000_china/Vi...         1       23  \n",
       "24941  /mnt/zshare/plants/Plant_Data/top5000_china/Vi...         1       23  \n",
       "24942  /mnt/zshare/plants/Plant_Data/top5000_china/Vi...         2       23  \n",
       "24943  /mnt/zshare/plants/Plant_Data/top5000_china/Vi...         0       23  \n",
       "\n",
       "[24944 rows x 5 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea943ff-1bdc-437d-a59e-e1ca71dce111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 两阶段植物识别系统：OrganClassifier + CLIP Species Retrieval\n",
    "# 适用于 plant_multitask_dataset.csv 格式数据，字段包括 path, organ, label\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== Step 1.0: 是否是植物分类器构建 ======\n",
    "class PlantBinaryDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "# ⚠️ 以下为样例，请替换为你自己的植物+非植物图像路径\n",
    "plant_images = df['path'].tolist()\n",
    "nonplant_images = [f\"/path/to/nonplant/img{i}.jpg\" for i in range(len(plant_images))]  # 需替换\n",
    "binary_image_paths = plant_images + nonplant_images\n",
    "binary_labels = [1]*len(plant_images) + [0]*len(nonplant_images)\n",
    "\n",
    "binary_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "binary_dataset = PlantBinaryDataset(binary_image_paths, binary_labels, binary_transform)\n",
    "binary_loader = DataLoader(binary_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "isplant_model = models.resnet18(pretrained=True)\n",
    "isplant_model.fc = nn.Linear(isplant_model.fc.in_features, 2)\n",
    "isplant_model = isplant_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(isplant_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"🚀 开始训练是否是植物分类器...\")\n",
    "for epoch in range(3):\n",
    "    isplant_model.train()\n",
    "    total, correct, loss_total = 0, 0, 0\n",
    "    loop = tqdm(binary_loader, desc=f\"PlantBinary Epoch {epoch+1}\")\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = isplant_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_total += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=loss_total/total)\n",
    "print(\"✅ 是否是植物模型训练完成\")\n",
    "\n",
    "# ====== Step 1: 初始化设备与数据 ======\n",
    "csv_path = \"/mnt/e/code/plants-classification-conda/plant_multitask_dataset.csv\"\n",
    "organ_json = \"/mnt/e/code/plants-classification-conda/organ_classes.json\"\n",
    "species_json = \"/mnt/e/code/plants-classification-conda/species_classes.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {v: k for k, v in species_classes.items()}\n",
    "id_to_organ = {v: k for k, v in organ_classes.items()}\n",
    "\n",
    "# ====== Step 2: 构建 OrganClassifier 数据集与模型 ======\n",
    "class OrganDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['path']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label_name = str(row['organ'])\n",
    "        if label_name not in organ_classes:\n",
    "            raise ValueError(f\"未知的器官标签: {label_name}\")\n",
    "        label = organ_classes[label_name]\n",
    "        return image, label\n",
    "\n",
    "# 数据增强\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "organ_dataset = OrganDataset(df, organ_transform)\n",
    "organ_loader = DataLoader(organ_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 构建模型\n",
    "organ_model = models.resnet18(pretrained=True)\n",
    "organ_model.fc = nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model = organ_model.to(device)\n",
    "\n",
    "# 训练器官分类器（可加早停）\n",
    "optimizer = torch.optim.Adam(organ_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "acc_list, loss_list = [], []\n",
    "for epoch in range(3):  # 可调轮数\n",
    "    organ_model.train()\n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0\n",
    "    loop = tqdm(organ_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = organ_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=running_loss/total)\n",
    "    acc_list.append(correct / total)\n",
    "    loss_list.append(running_loss / total)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc_list, label='Accuracy')\n",
    "plt.title('OrganClassifier Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_list, label='Loss', color='orange')\n",
    "plt.title('OrganClassifier Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== Step 3: 构建植物种类分类模型（替代 CLIP 检索） ======\n",
    "class SpeciesDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['path']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = int(row['species_id'])\n",
    "        return image, label\n",
    "\n",
    "species_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "species_dataset = SpeciesDataset(df, species_transform)\n",
    "species_loader = DataLoader(species_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "species_model = models.resnet50(pretrained=True)\n",
    "species_model.fc = nn.Linear(species_model.fc.in_features, len(species_classes))\n",
    "species_model = species_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(species_model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"🚀 开始训练植物种类分类模型...\")\n",
    "for epoch in range(3):\n",
    "    species_model.train()\n",
    "    total, correct, running_loss = 0, 0, 0\n",
    "    loop = tqdm(species_loader, desc=f\"Species Epoch {epoch+1}\")\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = species_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total, loss=running_loss/total)\n",
    "print(\"✅ 植物种类模型训练完成\")\n",
    "\n",
    "# ====== Step 4: 两阶段推理函数 ======\n",
    "def is_plant_clip(image_path):\n",
    "    # ⚠️ 替换为使用训练好的 isplant_model 判断是否为植物（1=植物, 0=非植物）\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = binary_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = isplant_model(img_tensor)\n",
    "        probs = torch.softmax(outputs, dim=-1).squeeze()\n",
    "        pred_label = torch.argmax(probs).item()\n",
    "        score = probs[1].item()  # 植物置信度\n",
    "    print(f\"🌱 是否植物预测: {pred_label}（植物概率: {score:.4f}）\")\n",
    "    return pred_label == 1\n",
    "    score = logits[0].item()  # 植物标签得分\n",
    "    print(\"🌱 是否植物得分:\", score)\n",
    "    return score >= threshold\n",
    "\n",
    "def predict_species(image_path, confidence_threshold=0.6):\n",
    "    if not is_plant_clip(image_path):\n",
    "        print(\"❌ 图像不是植物，请上传植物图像\")\n",
    "        return\n",
    "\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: 预测植物种类（使用softmax分类器）\n",
    "    species_tensor = species_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = species_model(species_tensor)\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "        top_prob, top_id = torch.max(probs, dim=0)\n",
    "\n",
    "    print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "    if top_prob.item() < confidence_threshold:\n",
    "        print(f\"⚠️ 无法确认植物种类（置信度 {top_prob.item():.4f}）\")\n",
    "        return\n",
    "\n",
    "    species = id_to_species[top_id.item()]\n",
    "    print(f\"🌿 识别为植物种类: {species}（置信度 {top_prob.item():.4f}）\")\n",
    "\n",
    "# ====== Step 5: 测试 ======\n",
    "# img_path = \"/mnt/e/code/plants-classification-conda/images/test_leaf.jpg\"\n",
    "# organ_pred, top_preds = predict_species(img_path)\n",
    "# print(\"Predicted organ:\", organ_pred)\n",
    "# for name, score in top_preds:\n",
    "#     print(f\"{name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884603fd-84c1-43a2-845b-49adb23d364b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672581e-376c-4db6-828f-19a1a3b50e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633c62d-c2af-403a-b27b-287b742b6e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544404a-e839-49b8-8789-cc012b3383a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29953165-806a-4bd5-ba9a-94028523f515",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 错误但可改版本：功能改进版1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99d199d7-7b2d-4899-9848-73d8c22e9edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_124/1218212356.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  organ_model.load_state_dict(torch.load(\"organ_classifier.pth\", map_location=device))\n",
      "/home/jmy/miniconda3/envs/plant310/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "Indexing bark: 100%|████████████████████████████████████████████████████| 2499/2499 [00:56<00:00, 44.38it/s]\n",
      "Indexing flower: 100%|██████████████████████████████████████████████████| 9974/9974 [03:46<00:00, 44.00it/s]\n",
      "Indexing fruit: 100%|███████████████████████████████████████████████████| 2498/2498 [00:56<00:00, 43.84it/s]\n",
      "Indexing leaf: 100%|████████████████████████████████████████████████████| 9973/9973 [03:47<00:00, 43.82it/s]\n"
     ]
    }
   ],
   "source": [
    "#################    功能1改进版     #############################\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "with open(organ_json, 'r') as f:\n",
    "    organ_classes = json.load(f)\n",
    "with open(species_json, 'r') as f:\n",
    "    species_classes = json.load(f)\n",
    "\n",
    "id_to_species = {int(v): k for k, v in species_classes.items()}\n",
    "id_to_organ = {int(v): k for k, v in organ_classes.items()}\n",
    "organ_classes = {str(k).strip().lower(): v for k, v in organ_classes.items()}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# organ model\n",
    "organ_model = resnet18(pretrained=True)\n",
    "organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.load_state_dict(torch.load(\"organ_classifier.pth\", map_location=device))\n",
    "organ_model = organ_model.to(device).eval()\n",
    "\n",
    "# CLIP model\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "# ==== Step 2: 构建 CLIP 检索索引 ====\n",
    "organ_index, organ_label_array = {}, {}\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    organ_df = df[df['organ'] == organ_name].reset_index(drop=True)\n",
    "    embeddings, labels = [], []\n",
    "    for _, row in tqdm(organ_df.iterrows(), total=len(organ_df), desc=f\"Indexing {organ_name}\"):\n",
    "        try:\n",
    "            image = preprocess(Image.open(row['image_path']).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = clip_model.encode_image(image)\n",
    "                emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.append(species_classes[row['label']])\n",
    "        except:\n",
    "            continue\n",
    "    if embeddings:\n",
    "        emb_arr = np.vstack(embeddings).astype(\"float32\")\n",
    "        index = faiss.IndexFlatIP(emb_arr.shape[1])\n",
    "        index.add(emb_arr)\n",
    "        organ_index[organ_id] = index\n",
    "        organ_label_array[organ_id] = np.array(labels)\n",
    "\n",
    "# ==== Step 3: 是否是植物判断（CLIP Zero-Shot） ====\n",
    "def is_plant_clip(image_path, threshold=0.3):\n",
    "    text_labels = [\"a photo of a plant\", \"a photo of an animal\", \"a photo of a person\", \"a photo of an object\"]\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        text_tokens = open_clip.tokenize(text_labels).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze()\n",
    "    score = logits[0].item()  # 植物标签得分\n",
    "    print(\"🌱 是否植物得分:\", score)\n",
    "    return score >= threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "87fd7a57-acfd-4f55-a348-d16056a8822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Step 4: 主推理函数（功能1改进版） ====\n",
    "def predict_species(image_path, topk=5):\n",
    "    if not is_plant_clip(image_path):\n",
    "        print(\"❌ 图像不是植物，请上传植物图像\")\n",
    "        return\n",
    "\n",
    "    # Step 1: 预测器官\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: 检索植物种类\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    index = organ_index.get(pred_organ_id)\n",
    "    labels = organ_label_array.get(pred_organ_id)\n",
    "    if index is None or labels is None:\n",
    "        print(\"❌ 无法进行器官分类或特征索引\")\n",
    "        return\n",
    "\n",
    "    D, I = index.search(query_np, topk)\n",
    "\n",
    "    result = []\n",
    "    for j, i in enumerate(I[0]):\n",
    "        label_id = int(labels[i])\n",
    "        species_name = id_to_species.get(label_id, None)\n",
    "        if species_name is None:\n",
    "            continue\n",
    "        # ✅ 功能1：判断预测的植物种类是否在训练集物种中（df 中）\n",
    "        if species_name not in df['label'].values:\n",
    "            print(f\"⚠️ 功能1判断：预测的植物种类 {species_name} 不在训练数据中，跳过\")\n",
    "            continue\n",
    "        result.append((species_name, float(D[0][j])))\n",
    "\n",
    "    if not result:\n",
    "        print(\"⚠️ 功能1判断：无法识别，预测植物种类不在训练数据中\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔍 识别为器官: {id_to_organ[pred_organ_id]}\")\n",
    "    for name, score in result:\n",
    "        print(f\"✅ {name}（得分: {score:.4f}）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e621d544-4158-4c32-9c86-7c5c6a428a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌱 是否植物得分: 0.4385930597782135\n",
      "🔍 识别为器官: fruit\n",
      "✅ Acer rubrum（得分: 0.8031）\n",
      "✅ Populus tremula（得分: 0.8012）\n",
      "✅ Caesalpinia pulcherrima（得分: 0.7981）\n",
      "✅ Acacia dealbata（得分: 0.7881）\n",
      "✅ Acer rubrum（得分: 0.7819）\n"
     ]
    }
   ],
   "source": [
    "predict_species(\"/mnt/e/code/plants-classification-conda/test_outside.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a925d2-12e7-43b8-bbd0-655aa48ce7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4ae055a0-3cb2-4c8a-8891-62ad58ebb1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/3161908934.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  organ_model.load_state_dict(torch.load(\"organ_classifier.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# ✅ 1. 设备\n",
    "'''device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ✅ 2. 加载类别映射\n",
    "with open(\"species_classes_top900.json\") as f:\n",
    "    species_classes = json.load(f)\n",
    "with open(\"organ_classes_top900.json\") as f:\n",
    "    organ_classes = json.load(f)\n",
    "\n",
    "id_to_species = {int(v): k for k, v in species_classes.items()}\n",
    "id_to_organ = {int(v): k for k, v in organ_classes.items()}\n",
    "\n",
    "# ✅ 3. 加载器官分类器\n",
    "organ_model = models.resnet18(pretrained=False)\n",
    "organ_model.fc = torch.nn.Linear(organ_model.fc.in_features, len(organ_classes))\n",
    "organ_model.load_state_dict(torch.load(\"organ_classifier.pth\", map_location=device))\n",
    "organ_model = organ_model.to(device).eval()\n",
    "\n",
    "# ✅ 4. 加载 CLIP\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "# ✅ 5. 图像转换\n",
    "organ_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ✅ 6. 加载 faiss index 和 label\n",
    "organ_index = {}\n",
    "organ_label_array = {}\n",
    "for organ_name, organ_id in organ_classes.items():\n",
    "    index_path = f\"clip_index/index_{organ_id}.index\"\n",
    "    label_path = f\"clip_index/labels_{organ_id}.npy\"\n",
    "    if os.path.exists(index_path) and os.path.exists(label_path):\n",
    "        organ_index[int(organ_id)] = faiss.read_index(index_path)\n",
    "        organ_label_array[int(organ_id)] = np.load(label_path)\n",
    "    else:\n",
    "        print(f\"\\u274c 缺失器官 {organ_name} 的索引或标签数组\")'''\n",
    "\n",
    "# ✅ 7. 新功能: 是否为植物：利用 CLIP zero-shot\n",
    "text_prompts = [\"a photo of a plant\", \"a photo of a car\", \"a photo of a dog\"]\n",
    "with torch.no_grad():\n",
    "    text_tokens = open_clip.tokenize(text_prompts).to(device)\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def is_plant_clip(image_path, threshold=0.2):\n",
    "    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feature = clip_model.encode_image(image)\n",
    "        img_feature /= img_feature.norm(dim=-1, keepdim=True)\n",
    "        similarity = (img_feature @ text_features.T).squeeze(0).cpu().numpy()\n",
    "    return similarity[0] > threshold, similarity\n",
    "\n",
    "# ✅ 8. 第二阶段分类\n",
    "\n",
    "def predict_species(image_path, known_hashes=None, topk=5):\n",
    "    is_plant, sim = is_plant_clip(image_path)\n",
    "    if not is_plant:\n",
    "        print(\"❌ 不是植物，请重新上传\")\n",
    "        return None\n",
    "\n",
    "    if known_hashes is not None:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "        if file_hash not in known_hashes:\n",
    "            print(\"⚠️ 图像未知，无法分类\")\n",
    "            return None\n",
    "\n",
    "    # Step 1: 器官预测\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = organ_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_organ_id = organ_model(img_tensor).argmax(1).item()\n",
    "\n",
    "    # Step 2: CLIP 向量检索\n",
    "    clip_img = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vec = clip_model.encode_image(clip_img)\n",
    "        query_vec = query_vec / query_vec.norm(dim=-1, keepdim=True)\n",
    "        query_np = query_vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    index = organ_index[pred_organ_id]\n",
    "    labels = organ_label_array[pred_organ_id]\n",
    "    D, I = index.search(query_np, topk)\n",
    "\n",
    "    result = []\n",
    "    for j, i in enumerate(I[0]):\n",
    "        label_id = int(labels[i])\n",
    "        if label_id not in id_to_species:\n",
    "            print(f\"⚠️ 识别结果的物种 ID: {label_id} 不在训练集物种中（未登记）\")\n",
    "            continue\n",
    "        species_name = id_to_species[label_id]\n",
    "        result.append((species_name, float(D[0][j])))\n",
    "\n",
    "    if not result:\n",
    "        print(\"⚠️ 功能1判断：该植物种类未在训练数据中，无法识别\")\n",
    "        return None\n",
    "\n",
    "    print(\"🔍 器官预测: \", id_to_organ[pred_organ_id])\n",
    "    for name, score in result:\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "    return id_to_organ[pred_organ_id], result\n",
    "\n",
    "# 例子调用\n",
    "# predict_species(\"/path/to/image.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d141b154-6c3c-4015-a6b5-188aeb4a31ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 不是植物，请重新上传\n"
     ]
    }
   ],
   "source": [
    "predict_species(\"/mnt/e/code/plants-classification-conda/test_900outside_ajuga_reptans.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32e1ac43-58bd-43bf-8ec6-896d1820171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(id_to_species.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f564606-7fd5-4810-8d43-d91599265adb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 数据检查-------路径、clip、index对应检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "94fb7cb4-df96-46e2-8bb8-9d9126cb05ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签数组 shape: (118893,)\n",
      "前10个标签ID: [295 295 295 ... 106 106 106]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118893"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_path = \"clip_index/labels_3.npy\"  # 替换为你的文件路径\n",
    "labels = np.load(label_path)\n",
    "\n",
    "print(\"标签数组 shape:\", labels.shape)\n",
    "print(\"前10个标签ID:\", labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "35cce2a6-1a65-4b43-92f2-5da4cf711371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Acacia dealbata',\n",
       " 1: 'Acer negundo',\n",
       " 2: 'Acer rubrum',\n",
       " 3: 'Berberis japonica',\n",
       " 4: 'Buddleja davidii',\n",
       " 5: 'Caesalpinia pulcherrima',\n",
       " 6: 'Campsis radicans',\n",
       " 7: 'Elaeagnus pungens',\n",
       " 8: 'Equisetum arvense',\n",
       " 9: 'Malva sylvestris',\n",
       " 10: 'Populus tremula',\n",
       " 11: 'Prunus virginiana',\n",
       " 12: 'Pyracantha coccinea',\n",
       " 13: 'Reynoutria japonica',\n",
       " 14: 'Rhus typhina',\n",
       " 15: 'Ricinus communis',\n",
       " 16: 'Rumex crispus',\n",
       " 17: 'Sambucus racemosa',\n",
       " 18: 'Scandosorbus intermedia',\n",
       " 19: 'Senecio vulgaris',\n",
       " 20: 'Sonchus arvensis',\n",
       " 21: 'Sorbus aucuparia',\n",
       " 22: 'Viburnum opulus',\n",
       " 23: 'Viburnum tinus',\n",
       " 24: 'Vitex agnus-castus'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设已经加载了 species_classes：\n",
    "# id_to_species = {int(v): k for k, v in species_classes.items()}\n",
    "\n",
    "#species_names = [id_to_species[label] for label in labels[:10]]\n",
    "#print(\"前10个植物名:\", id_to_species)\n",
    "id_to_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11e6a0ce-a57f-42c9-ac92-f12511cdd170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量维度: 768\n",
      "向量数量: 118893\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index_path = \"clip_index/index_3.index\"  # 替换为你的路径\n",
    "index = faiss.read_index(index_path)\n",
    "\n",
    "print(\"向量维度:\", index.d)\n",
    "print(\"向量数量:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "55425df5-a24e-4d20-8a09-2438d66c6092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧩 Checking pair: index_0 & labels_0\n",
      "✅ Index vectors: 23064, Dimension: 768\n",
      "✅ Labels shape: (23064,)\n",
      "🔎 Sample label -> species name:\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "\n",
      "🧩 Checking pair: index_1 & labels_1\n",
      "✅ Index vectors: 118698, Dimension: 768\n",
      "✅ Labels shape: (118698,)\n",
      "🔎 Sample label -> species name:\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "\n",
      "🧩 Checking pair: index_2 & labels_2\n",
      "✅ Index vectors: 28454, Dimension: 768\n",
      "✅ Labels shape: (28454,)\n",
      "🔎 Sample label -> species name:\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "\n",
      "🧩 Checking pair: index_3 & labels_3\n",
      "✅ Index vectors: 118893, Dimension: 768\n",
      "✅ Labels shape: (118893,)\n",
      "🔎 Sample label -> species name:\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n",
      "  - 295 → ❌ Not found in species_classes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#找到对应的index！！！！！！！！！\n",
    "index_folder = \"/mnt/e/code/plants-classification-conda/clip_index\"\n",
    "species_json = \"species_classes_top997.json\"\n",
    "\n",
    "# 1. 加载 species 映射（int ID -> 拉丁名）\n",
    "with open(species_json, \"r\") as f:\n",
    "    species_classes = json.load(f)\n",
    "id_to_species = {int(v): k for k, v in species_classes.items()}\n",
    "\n",
    "# 2. 遍历 index & label 文件\n",
    "for i in range(10):  # 最多检查10对\n",
    "    index_path = os.path.join(index_folder, f\"index_{i}.index\")\n",
    "    label_path = os.path.join(index_folder, f\"labels_{i}.npy\")\n",
    "    \n",
    "    if not os.path.exists(index_path) or not os.path.exists(label_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🧩 Checking pair: index_{i} & labels_{i}\")\n",
    "\n",
    "    # 加载 FAISS index\n",
    "    try:\n",
    "        index = faiss.read_index(index_path)\n",
    "        num_vectors = index.ntotal\n",
    "        dim = index.d\n",
    "        print(f\"✅ Index vectors: {num_vectors}, Dimension: {dim}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load index_{i}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 加载标签\n",
    "    try:\n",
    "        labels = np.load(label_path)\n",
    "        print(f\"✅ Labels shape: {labels.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load labels_{i}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 核对向量数和标签数是否一致\n",
    "    if len(labels) != num_vectors:\n",
    "        print(\"⚠️ 向量数量与标签数量不匹配！\")\n",
    "        continue\n",
    "\n",
    "    # 显示前几个标签对应的物种名\n",
    "    print(\"🔎 Sample label -> species name:\")\n",
    "    for label_id in labels[:5]:\n",
    "        species_name = id_to_species.get(int(label_id), \"❌ Not found in species_classes\")\n",
    "        print(f\"  - {label_id} → {species_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f34980-9f06-4edb-a941-80bb2a1f5b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plant310)",
   "language": "python",
   "name": "plant310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
